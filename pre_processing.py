## Conventional Preprocessing

# 1. ObsPy ì„¤ì¹˜
!pip install obspy

# 2. ìˆ˜ë™ìœ¼ë¡œ ëŸ°íƒ€ì„ ì¬ì‹œì‘
# ë©”ë‰´: Runtime â†’ Restart runtime

# 3. ì¬ì‹œì‘ í›„ import í…ŒìŠ¤íŠ¸
import obspy
print(f"âœ… ObsPy ë²„ì „: {obspy.__version__}")



import numpy as np
import obspy
from scipy import signal
import warnings
warnings.filterwarnings('ignore')

print("=== í¬ë˜ì‹œ ì•ˆì „ ì „í†µì ì¸ ì§€ì§„íŒŒ ì „ì²˜ë¦¬ 6ë‹¨ê³„ ===")
print("Demultiplexing â†’ Trace Editing â†’ Gain Recovery â†’ Filtering â†’ Deconvolution â†’ CMP Gather")

# ë°ì´í„° ë¡œë”©
print("\nì›ì‹œ ë°ì´í„° ë¡œë”©...")
stream = obspy.read("ANMO_sample.mseed")
print(f"âœ… ë¡œë”© ì™„ë£Œ: {len(stream)}ê°œ íŠ¸ë ˆì´ìŠ¤")

# ì›ë³¸ ë°ì´í„° ë°±ì—…
original_stream = stream.copy()



print("\n=== 3ì±„ë„ ë°ì´í„° ì‹œê°í™” ì¤€ë¹„ ===")

# ì‹œê°„ ì¶• ìƒì„±
time_axis = {}
for i, trace in enumerate(stream):
    sampling_rate = trace.stats.sampling_rate
    num_samples = len(trace.data)
    duration = num_samples / sampling_rate
    time_axis[trace.stats.channel] = np.linspace(0, duration, num_samples)

    print(f"{trace.stats.channel} ì±„ë„:")
    print(f"  ì‹œê°„ ë²”ìœ„: 0 ~ {duration:.1f}ì´ˆ")
    print(f"  ë°ì´í„° ë²”ìœ„: {trace.data.min():.1f} ~ {trace.data.max():.1f}")

print("\nì‹œê°í™” ì½”ë“œ (matplotlib ì‚¬ìš© ì‹œ):")

import matplotlib.pyplot as plt

fig, axes = plt.subplots(3, 1, figsize=(12, 8))
for i, trace in enumerate(stream):
    channel = trace.stats.channel
    time = time_axis[channel]
    axes[i].plot(time, trace.data)
    axes[i].set_title(f'{channel} ì±„ë„')
    axes[i].set_ylabel('ì§„í­')
    if i == 2:
        axes[i].set_xlabel('ì‹œê°„ (ì´ˆ)')
plt.tight_layout()
plt.show()



# =====================================================
# 1ë‹¨ê³„: Demultiplexing (ì—­ë‹¤ì¤‘í™”)
# ì´ë¯¸ ë°ì´í„°ê°€ 3ê°œì˜ íŠ¸ë ˆì´ìŠ¤ì—ì„œ BH1, BH2, BHZë¡œ ë¶„ë¦¬ë˜ì–´ìˆê¸° ë•Œë¬¸ì— í™•ì¸í•˜ê³  ì •ë¦¬í•˜ëŠ” ë‹¨ê³„ë¡œ ì—¬ê¸°ë©´ ëœë‹¤
# =====================================================
print("\nğŸ”¸ 1ë‹¨ê³„: Demultiplexing (ì—­ë‹¤ì¤‘í™”)")
print("- ë‹¤ì±„ë„ ì§€ì§„íŒŒ ë°ì´í„°ë¥¼ ê°œë³„ ì±„ë„ë¡œ ë¶„ë¦¬")

demux_channels = {}
for i, trace in enumerate(stream):
    channel_id = trace.stats.channel
    demux_channels[channel_id] = {
        'trace': trace,
        'network': trace.stats.network,
        'station': trace.stats.station,
        'channel': trace.stats.channel,
        'sampling_rate': trace.stats.sampling_rate,
        'npts': trace.stats.npts,
        'starttime': trace.stats.starttime
    }
    print(f"  ğŸ“Š {channel_id}: {trace.stats.sampling_rate}Hz, {trace.stats.npts} samples")

print(f"âœ… 1ë‹¨ê³„ ì™„ë£Œ: {len(demux_channels)}ê°œ ì±„ë„ ë¶„ë¦¬")



# =====================================================
# 2ë‹¨ê³„: Trace Editing (íŠ¸ë ˆì´ìŠ¤ í¸ì§‘)
# =====================================================
print("\nğŸ”¸ 2ë‹¨ê³„: Trace Editing (íŠ¸ë ˆì´ìŠ¤ í¸ì§‘)")
print("- ë¶ˆëŸ‰ ë°ì´í„° ì œê±° ë° í’ˆì§ˆ ê´€ë¦¬")

edited_channels = {}
for channel_id, ch_data in demux_channels.items():
    trace = ch_data['trace']
    data = trace.data.copy()

    print(f"  ğŸ” {channel_id} í’ˆì§ˆ ê²€ì‚¬...")

    # Dead trace ê²€ì‚¬
    if np.all(data == 0) or np.var(data) < 1e-12:
        print(f"    âŒ Dead trace - ì œê±°")
        continue

    # NaN/Inf ê°’ ê²€ì‚¬
    if np.any(np.isnan(data)) or np.any(np.isinf(data)):
        print(f"    âš ï¸ NaN/Inf ê°’ ë°œê²¬ - ë³´ì •")
        # NaNì„ 0ìœ¼ë¡œ, Infë¥¼ í´ë¦¬í•‘
        data = np.nan_to_num(data, nan=0.0, posinf=np.max(data[np.isfinite(data)]),
                            neginf=np.min(data[np.isfinite(data)]))

    # ìŠ¤íŒŒì´í¬ ì œê±° (Z-score > 5)
    z_scores = np.abs((data - np.mean(data)) / (np.std(data) + 1e-10))
    spike_count = np.sum(z_scores > 5)
    if spike_count > 0:
        print(f"    âš ï¸ {spike_count}ê°œ ìŠ¤íŒŒì´í¬ ì œê±°")
        spike_mask = z_scores > 5
        # ìŠ¤íŒŒì´í¬ë¥¼ ì£¼ë³€ ê°’ì˜ í‰ê· ìœ¼ë¡œ ëŒ€ì²´
        for idx in np.where(spike_mask)[0]:
            if idx > 0 and idx < len(data) - 1:
                data[idx] = (data[idx-1] + data[idx+1]) / 2

    # í¸ì§‘ëœ ë°ì´í„° ì €ì¥
    edited_trace = trace.copy()
    edited_trace.data = data
    edited_channels[channel_id] = edited_trace
    print(f"    âœ… í’ˆì§ˆ ê²€ì‚¬ í†µê³¼")

print(f"âœ… 2ë‹¨ê³„ ì™„ë£Œ: {len(edited_channels)}ê°œ ì±„ë„ ìœ ì§€")



print("=== í¬í™” ë³´ê°„ì´ ì‹¤íŒ¨í•œ ì´ìœ  ë¶„ì„ ===")

# í¬í™” ë³´ê°„ ë¡œì§ ë¶„ì„
for channel_id, ch_data in demux_channels.items():
    data = ch_data['trace'].data

    print(f"\nğŸ” {channel_id} í¬í™” ë¶„ì„:")

    max_val = np.max(np.abs(data))
    saturation_threshold = max_val * 0.95
    saturated_mask = np.abs(data) >= saturation_threshold
    saturated_indices = np.where(saturated_mask)[0]

    print(f"    ìµœëŒ€ê°’: {max_val:.2f}")
    print(f"    í¬í™” ì„ê³„ê°’: {saturation_threshold:.2f}")
    print(f"    í¬í™” ê°’ ê°œìˆ˜: {len(saturated_indices)}")

    if len(saturated_indices) > 0:
        print(f"    í¬í™” ê°’ ìœ„ì¹˜: {saturated_indices[:10]}...")  # ì²˜ìŒ 10ê°œë§Œ

        # ë³´ê°„ ì‹¤íŒ¨ ì›ì¸ ë¶„ì„
        edge_count = 0
        cluster_count = 0
        successful_count = 0

        for idx in saturated_indices:
            if idx <= 5 or idx >= len(data) - 5:
                edge_count += 1
            else:
                # ì£¼ë³€ì—ë„ í¬í™” ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
                before_vals = data[idx-5:idx][~saturated_mask[idx-5:idx]]
                after_vals = data[idx+1:idx+6][~saturated_mask[idx+1:idx+6]]

                if len(before_vals) == 0 or len(after_vals) == 0:
                    cluster_count += 1
                else:
                    successful_count += 1

        print(f"    ë³´ê°„ ì‹¤íŒ¨ ì›ì¸:")
        print(f"      ê²½ê³„ ê·¼ì²˜: {edge_count}ê°œ")
        print(f"      í¬í™” í´ëŸ¬ìŠ¤í„°: {cluster_count}ê°œ")
        print(f"      ë³´ê°„ ê°€ëŠ¥: {successful_count}ê°œ")

        # ì‹¤ì œ ë³´ê°„ ê°€ëŠ¥í•œ ê²ƒë§Œ ì²˜ë¦¬
        if successful_count > 0:
            print(f"    ğŸ’¡ ì‹¤ì œë¡œëŠ” {successful_count}ê°œ ë³´ê°„ ê°€ëŠ¥í–ˆìŒ!")



print("\n=== ê°œì„ ëœ í¬í™” ì²˜ë¦¬ ===")

# ë” ë˜‘ë˜‘í•œ í¬í™” ì²˜ë¦¬
really_improved_edited = {}

for channel_id, ch_data in demux_channels.items():
    trace = ch_data['trace']
    data = trace.data.copy()

    print(f"\nğŸ› ï¸ {channel_id} ê°œì„ ëœ í¸ì§‘:")

    total_edits = 0

    # 1. ë” ê´€ëŒ€í•œ í¬í™” ì²˜ë¦¬
    max_val = np.max(np.abs(data))
    saturation_threshold = max_val * 0.98  # ë” ì—„ê²©í•œ ê¸°ì¤€
    saturated_indices = np.where(np.abs(data) >= saturation_threshold)[0]

    if len(saturated_indices) > 0:
        print(f"    ğŸ”´ {len(saturated_indices)}ê°œ í¬í™” ê°’ ë°œê²¬")

        saturation_edits = 0
        for idx in saturated_indices:
            # ë” ì‘ì€ ìœˆë„ìš°ë¡œ ë³´ê°„ ì‹œë„
            if idx > 2 and idx < len(data) - 2:
                # í¬í™”ë˜ì§€ ì•Šì€ ê°€ì¥ ê°€ê¹Œìš´ ê°’ë“¤ ì°¾ê¸°
                left_val = None
                right_val = None

                # ì™¼ìª½ íƒìƒ‰
                for i in range(idx-1, max(0, idx-10), -1):
                    if np.abs(data[i]) < saturation_threshold:
                        left_val = data[i]
                        break

                # ì˜¤ë¥¸ìª½ íƒìƒ‰
                for i in range(idx+1, min(len(data), idx+10)):
                    if np.abs(data[i]) < saturation_threshold:
                        right_val = data[i]
                        break

                # ë³´ê°„ ìˆ˜í–‰
                if left_val is not None and right_val is not None:
                    data[idx] = (left_val + right_val) / 2
                    saturation_edits += 1
                elif left_val is not None:
                    data[idx] = left_val * 0.9  # ì•½ê°„ ê°ì†Œ
                    saturation_edits += 1
                elif right_val is not None:
                    data[idx] = right_val * 0.9  # ì•½ê°„ ê°ì†Œ
                    saturation_edits += 1

        print(f"    âœ… {saturation_edits}ê°œ í¬í™” ê°’ ë³´ê°„")
        total_edits += saturation_edits

    # 2. ë” íš¨ê³¼ì ì¸ ë“œë¦¬í”„íŠ¸ ì œê±°
    window_size = 400  # 10ì´ˆ ìœˆë„ìš°ë¡œ ì¤„ì„
    if len(data) > window_size:
        # ë‹¤í•­ì‹ detrend (2ì°¨)
        x = np.arange(len(data))
        coeffs = np.polyfit(x, data, 2)  # 2ì°¨ ë‹¤í•­ì‹
        trend = np.polyval(coeffs, x)
        detrended_data = data - trend

        # ë“œë¦¬í”„íŠ¸ ì œê±° íš¨ê³¼
        original_drift = np.std([np.mean(data[i:i+400]) for i in range(0, len(data)-400, 400)])
        new_drift = np.std([np.mean(detrended_data[i:i+400]) for i in range(0, len(detrended_data)-400, 400)])

        print(f"    ğŸ“‰ 2ì°¨ ë‹¤í•­ì‹ detrend: {original_drift:.2f} â†’ {new_drift:.2f}")
        data = detrended_data

    # 3. ìŠ¤íŒŒì´í¬ ì œê±° (2.5Ïƒ ê¸°ì¤€ìœ¼ë¡œ ë” ì—„ê²©í•˜ê²Œ)
    z_scores = np.abs((data - np.mean(data)) / np.std(data))
    spike_mask = z_scores > 2.5
    spike_count = np.sum(spike_mask)

    if spike_count > 0:
        print(f"    âš¡ {spike_count}ê°œ ìŠ¤íŒŒì´í¬ ì œê±° (2.5Ïƒ ê¸°ì¤€)")
        for idx in np.where(spike_mask)[0]:
            if idx > 1 and idx < len(data) - 1:
                # ì¤‘ì•™ê°’ í•„í„° ì ìš©
                data[idx] = np.median(data[idx-1:idx+2])
        total_edits += spike_count

    # 4. ì¶”ê°€: ê³ ì£¼íŒŒ ë…¸ì´ì¦ˆ ê°ì†Œ
    # ê°„ë‹¨í•œ 3ì  ì´ë™í‰ê· ìœ¼ë¡œ ê³ ì£¼íŒŒ ë…¸ì´ì¦ˆ ê°ì†Œ
    smoothed_data = np.convolve(data, [0.25, 0.5, 0.25], mode='same')
    noise_reduction = np.std(data - smoothed_data)

    if noise_reduction > np.std(data) * 0.1:  # 10% ì´ìƒì˜ ë…¸ì´ì¦ˆ
        print(f"    ğŸ”‡ ê³ ì£¼íŒŒ ë…¸ì´ì¦ˆ ê°ì†Œ: {noise_reduction:.2f}")
        data = smoothed_data

    # ê²°ê³¼ ì €ì¥
    edited_trace = trace.copy()
    edited_trace.data = data
    really_improved_edited[channel_id] = edited_trace

    print(f"    âœ… ì´ í¸ì§‘: {total_edits}ê°œ ê°’ ìˆ˜ì •")

print(f"\nğŸ¯ ì§„ì§œ ê°œì„ ëœ í¸ì§‘ ê²°ê³¼!")



# =====================================================
# 3ë‹¨ê³„: Gain Recovery (ì´ë“ ë³µêµ¬)
# =====================================================
print("\nğŸ”¸ 3ë‹¨ê³„: Gain Recovery (ì´ë“ ë³µêµ¬)")
print("- ê¸°ë¡ ì‹œ ì ìš©ëœ ì´ë“ì„ ë³´ìƒí•˜ì—¬ ì›ë˜ ì§„í­ ë³µì›")

gain_recovered = {}
for channel_id, trace in edited_channels.items():
    data = trace.data.copy()

    print(f"  âš¡ {channel_id} ì´ë“ ë³µêµ¬...")

    # 1. ê¸°í•˜í•™ì  í™•ì‚° ë³´ì • (r^-1 ë²•ì¹™)
    # ì¼ë°˜ì ì¸ ê±°ë¦¬ 100km ê°€ì •
    distance_km = 100
    geometric_factor = distance_km  # ê±°ë¦¬ë¹„ë¡€ ë³´ì •

    # 2. ë§¤ì²´ ê°ì‡  ë³´ì • (Q factor ë³´ì •)
    # ì£¼íŒŒìˆ˜ ì˜ì—­ì—ì„œ ê°ì‡  ë³´ì •
    Q_factor = 200  # ì¼ë°˜ì ì¸ Qê°’
    velocity_km_s = 3.5  # SíŒŒ ì†ë„

    # FFTë¡œ ì£¼íŒŒìˆ˜ ì˜ì—­ ë³€í™˜
    dt = 1.0 / trace.stats.sampling_rate
    freqs = np.fft.fftfreq(len(data), dt)
    fft_data = np.fft.fft(data)

    # ê°ì‡  ë³´ì • ì ìš© (0 ì£¼íŒŒìˆ˜ ì œì™¸)
    attenuation_correction = np.ones_like(freqs)
    non_zero_freq = freqs != 0
    attenuation_correction[non_zero_freq] = np.exp(
        np.pi * np.abs(freqs[non_zero_freq]) * distance_km / (Q_factor * velocity_km_s)
    )

    # ë³´ì • ì ìš© ë° ì—­ë³€í™˜
    corrected_fft = fft_data * attenuation_correction
    corrected_data = np.real(np.fft.ifft(corrected_fft))

    # 3. ê¸°í•˜í•™ì  í™•ì‚° ë³´ì • ì ìš©
    final_data = corrected_data * geometric_factor

    # 4. ê³„ê¸° ì‘ë‹µ ì œê±° (ê°„ë‹¨í•œ ê³ ì—­í†µê³¼)
    # ë§¤ìš° ë‚®ì€ ì£¼íŒŒìˆ˜ ì„±ë¶„ ì œê±° (0.01Hz ì´í•˜)
    if trace.stats.sampling_rate > 0.02:  # ë‚˜ì´í€´ìŠ¤íŠ¸ ì¡°ê±´
        # ìˆ˜ë™ìœ¼ë¡œ ê³ ì—­í†µê³¼ í•„í„° êµ¬í˜„ (scipy ì‚¬ìš©)
        nyquist = trace.stats.sampling_rate / 2
        low_cutoff = min(0.01, nyquist * 0.01)  # ì•ˆì „í•œ cutoff

        try:
            b, a = signal.butter(2, low_cutoff / nyquist, btype='high')
            final_data = signal.filtfilt(b, a, final_data)
        except:
            print(f"    âš ï¸ ê³ ì—­í†µê³¼ í•„í„° ì‹¤íŒ¨ - ê±´ë„ˆëœ€")

    # ê²°ê³¼ ì €ì¥
    recovered_trace = trace.copy()
    recovered_trace.data = final_data
    gain_recovered[channel_id] = recovered_trace

    print(f"    âœ… ì´ë“ ë³µêµ¬ ì™„ë£Œ (ì¦í­: {geometric_factor:.1f}x)")

print(f"âœ… 3ë‹¨ê³„ ì™„ë£Œ: {len(gain_recovered)}ê°œ ì±„ë„ ì´ë“ ë³µêµ¬")



# =====================================================
# 4ë‹¨ê³„: Filtering (í•„í„°ë§)
# =====================================================
print("\nğŸ”¸ 4ë‹¨ê³„: Filtering (í•„í„°ë§)")
print("- ì£¼íŒŒìˆ˜ ì˜ì—­ì—ì„œ ë…¸ì´ì¦ˆ ì œê±°")

filtered_channels = {}
for channel_id, trace in gain_recovered.items():
    data = trace.data.copy()

    print(f"  ğŸ›ï¸ {channel_id} í•„í„°ë§...")

    # 1. ì„ í˜• íŠ¸ë Œë“œ ì œê±° (ìˆ˜ë™ êµ¬í˜„)
    x = np.arange(len(data))
    if len(data) > 1:
        slope = np.sum((x - np.mean(x)) * (data - np.mean(data))) / np.sum((x - np.mean(x))**2)
        intercept = np.mean(data) - slope * np.mean(x)
        trend = slope * x + intercept
        detrended_data = data - trend
    else:
        detrended_data = data

    # 2. ë°´ë“œíŒ¨ìŠ¤ í•„í„° (1-20Hz) - scipy ì§ì ‘ ì‚¬ìš©
    sampling_rate = trace.stats.sampling_rate
    nyquist = sampling_rate / 2

    # ì•ˆì „í•œ ì£¼íŒŒìˆ˜ ë²”ìœ„ ì„¤ì •
    low_freq = min(1.0, nyquist * 0.1)
    high_freq = min(20.0, nyquist * 0.9)

    if low_freq < high_freq and nyquist > low_freq:
        try:
            # ë²„í„°ì›ŒìŠ¤ ë°´ë“œíŒ¨ìŠ¤ í•„í„°
            b, a = signal.butter(4, [low_freq/nyquist, high_freq/nyquist], btype='band')
            bandpass_data = signal.filtfilt(b, a, detrended_data)
            print(f"    âœ… ë°´ë“œíŒ¨ìŠ¤ í•„í„° ì ìš©: {low_freq:.1f}-{high_freq:.1f}Hz")
        except Exception as e:
            print(f"    âš ï¸ ë°´ë“œíŒ¨ìŠ¤ í•„í„° ì‹¤íŒ¨: {e}")
            bandpass_data = detrended_data
    else:
        print(f"    âš ï¸ ë¶€ì ì ˆí•œ ì£¼íŒŒìˆ˜ ë²”ìœ„ - í•„í„° ê±´ë„ˆëœ€")
        bandpass_data = detrended_data

    # 3. ë…¸ì¹˜ í•„í„° (60Hz ì „ë ¥ì„  ê°„ì„­ ì œê±°)
    if sampling_rate > 120:  # ë‚˜ì´í€´ìŠ¤íŠ¸ ì¡°ê±´
        try:
            notch_freq = 60.0
            Q = 30
            b, a = signal.iirnotch(notch_freq, Q, sampling_rate)
            notched_data = signal.filtfilt(b, a, bandpass_data)
            print(f"    âœ… ë…¸ì¹˜ í•„í„° ì ìš©: {notch_freq}Hz")
        except Exception as e:
            print(f"    âš ï¸ ë…¸ì¹˜ í•„í„° ì‹¤íŒ¨: {e}")
            notched_data = bandpass_data
    else:
        notched_data = bandpass_data

    # ê²°ê³¼ ì €ì¥
    filtered_trace = trace.copy()
    filtered_trace.data = notched_data
    filtered_channels[channel_id] = filtered_trace

print(f"âœ… 4ë‹¨ê³„ ì™„ë£Œ: {len(filtered_channels)}ê°œ ì±„ë„ í•„í„°ë§")



# =====================================================
# 5ë‹¨ê³„: Deconvolution (ì—­ì»¨ë³¼ë£¨ì…˜)
# =====================================================
print("\nğŸ”¸ 5ë‹¨ê³„: Deconvolution (ì—­ì»¨ë³¼ë£¨ì…˜)")
print("- ì§€ì§„íŒŒ ì „íŒŒ ê³¼ì •ì—ì„œ ë°œìƒí•œ íŒŒí˜• ì™œê³¡ ë³´ì •")

deconvolved_channels = {}
for channel_id, trace in filtered_channels.items():
    data = trace.data.copy()

    print(f"  ğŸ”„ {channel_id} ì—­ì»¨ë³¼ë£¨ì…˜...")

    # 1. ì˜ˆì¸¡ ì—­ì»¨ë³¼ë£¨ì…˜ (ê°„ë‹¨í•œ í˜•íƒœ)
    # ìê¸°ìƒê´€í•¨ìˆ˜ ê¸°ë°˜ ì˜ˆì¸¡ í•„í„°
    autocorr_length = min(100, len(data) // 10)  # ì•ˆì „í•œ ê¸¸ì´

    if len(data) > autocorr_length * 2:
        # ìê¸°ìƒê´€í•¨ìˆ˜ ê³„ì‚°
        autocorr = np.correlate(data, data, mode='full')
        center = len(autocorr) // 2
        autocorr = autocorr[center:center + autocorr_length]

        # ì˜ˆì¸¡ í•„í„° ì„¤ê³„ (ê°„ë‹¨í•œ í˜•íƒœ)
        if len(autocorr) > 1 and np.std(autocorr) > 0:
            # ì •ê·œí™”ëœ ìê¸°ìƒê´€
            autocorr = autocorr / autocorr[0]

            # ê°„ë‹¨í•œ ì˜ˆì¸¡ í•„í„° (2ì°¨)
            if len(autocorr) >= 3:
                pred_filter = np.array([1, -autocorr[1], autocorr[2]/2])
            else:
                pred_filter = np.array([1, -0.5])
        else:
            pred_filter = np.array([1])

        # ì˜ˆì¸¡ ì—­ì»¨ë³¼ë£¨ì…˜ ì ìš©
        try:
            deconv_data = signal.lfilter(pred_filter, [1], data)
        except:
            deconv_data = data
    else:
        deconv_data = data

    # 2. ìŠ¤íŒŒì´í‚¹ ì—­ì»¨ë³¼ë£¨ì…˜ íš¨ê³¼ (ê³ ì£¼íŒŒ ì„±ë¶„ ê°•í™”)
    # 1ì°¨ ë¯¸ë¶„ìœ¼ë¡œ ê³ ì£¼íŒŒ ê°•í™”
    diff_data = np.diff(deconv_data, prepend=deconv_data[0])

    # ì›ë³¸ê³¼ ë¯¸ë¶„ì˜ ê°€ì¤‘í•© (70% ì›ë³¸ + 30% ê³ ì£¼íŒŒ ê°•í™”)
    enhanced_data = 0.7 * deconv_data + 0.3 * diff_data

    # ê²°ê³¼ ì €ì¥
    deconv_trace = trace.copy()
    deconv_trace.data = enhanced_data
    deconvolved_channels[channel_id] = deconv_trace

    print(f"    âœ… ì—­ì»¨ë³¼ë£¨ì…˜ ì™„ë£Œ")

print(f"âœ… 5ë‹¨ê³„ ì™„ë£Œ: {len(deconvolved_channels)}ê°œ ì±„ë„ ì—­ì»¨ë³¼ë£¨ì…˜")



# =====================================================
# 6ë‹¨ê³„: CMP Gather (ê³µí†µ ì¤‘ì  ì§‘í•©)
# =====================================================
print("\nğŸ”¸ 6ë‹¨ê³„: CMP Gather (ê³µí†µ ì¤‘ì  ì§‘í•©)")
print("- ê°™ì€ ì§€í•˜ ì ì„ ë°˜ì‚¬í•œ ì‹ í˜¸ë“¤ì„ ê·¸ë£¹í™”")

# ì±„ë„ë³„ë¡œ ê·¸ë£¹í™” (Z, N, E ì„±ë¶„)
gathered_channels = {}
channel_groups = {
    'vertical': [],    # Z ì„±ë¶„
    'horizontal_1': [], # N, 1 ì„±ë¶„
    'horizontal_2': []  # E, 2 ì„±ë¶„
}

for channel_id, trace in deconvolved_channels.items():
    if 'Z' in channel_id:
        channel_groups['vertical'].append((channel_id, trace))
    elif 'N' in channel_id or '1' in channel_id:
        channel_groups['horizontal_1'].append((channel_id, trace))
    elif 'E' in channel_id or '2' in channel_id:
        channel_groups['horizontal_2'].append((channel_id, trace))

# ê° ê·¸ë£¹ë³„ë¡œ ëŒ€í‘œ ì±„ë„ ì„ íƒ (ë˜ëŠ” ìŠ¤íƒ)
for group_name, traces in channel_groups.items():
    if traces:
        if len(traces) == 1:
            # ë‹¨ì¼ ì±„ë„
            channel_id, trace = traces[0]
            gathered_channels[f"{group_name}_{channel_id}"] = trace
            print(f"  ğŸ“ {group_name}: {channel_id} ì„ íƒ")
        else:
            # ì—¬ëŸ¬ ì±„ë„ì´ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ë§Œ ì„ íƒ (ë˜ëŠ” ìŠ¤íƒ ê°€ëŠ¥)
            channel_id, trace = traces[0]
            gathered_channels[f"{group_name}_{channel_id}"] = trace
            print(f"  ğŸ“ {group_name}: {channel_id} ì„ íƒ ({len(traces)}ê°œ ì¤‘)")

print(f"âœ… 6ë‹¨ê³„ ì™„ë£Œ: {len(gathered_channels)}ê°œ ê·¸ë£¹ í˜•ì„±")



# =====================================================
# ìµœì¢… ê²°ê³¼
# =====================================================
print("\nğŸ‰ ì „í†µì ì¸ ì§€ì§„íŒŒ ì „ì²˜ë¦¬ 6ë‹¨ê³„ ì™„ë£Œ!")
print("="*50)
print("ì²˜ë¦¬ ê²°ê³¼:")
for step, count in [
    ("1. Demultiplexing", len(demux_channels)),
    ("2. Trace Editing", len(edited_channels)),
    ("3. Gain Recovery", len(gain_recovered)),
    ("4. Filtering", len(filtered_channels)),
    ("5. Deconvolution", len(deconvolved_channels)),
    ("6. CMP Gather", len(gathered_channels))
]:
    print(f"  {step}: {count}ê°œ ì±„ë„")

print("\nìµœì¢… ì¶œë ¥:")
final_processed_data = {}
for channel_name, trace in gathered_channels.items():
    final_processed_data[channel_name] = {
        'data': trace.data,
        'sampling_rate': trace.stats.sampling_rate,
        'channel': trace.stats.channel,
        'length': len(trace.data)
    }
    print(f"  ğŸ”¸ {channel_name}: {len(trace.data)} samples @ {trace.stats.sampling_rate}Hz")

print("\nâœ… ì „í†µì ì¸ ì „ì²˜ë¦¬ ì™„ë£Œ - ë”¥ëŸ¬ë‹ ì „ì²˜ë¦¬ ë‹¨ê³„ë¡œ ì§„í–‰ ê°€ëŠ¥!")



## 2. Preprocessing for Deep Learning

import numpy as np
import pandas as pd
from datetime import datetime, timedelta

print("ğŸš€ ë”¥ëŸ¬ë‹ìš© ì§€ì§„íŒŒ ì „ì²˜ë¦¬ ì‹œì‘!")
print("="*60)

# ============================================================================
# 1ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ ë° ê²€ì¦
# ============================================================================
print("\nğŸ“‹ 1ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ ë° ê²€ì¦")

# ì „ì²˜ë¦¬ëœ ì§€ì§„íŒŒ ë°ì´í„° í™•ì¸
print("ì „ì²˜ë¦¬ëœ ì§€ì§„íŒŒ ë°ì´í„°:")
for channel_name, data_info in final_processed_data.items():
    print(f"  ğŸ”¸ {channel_name}: {data_info['length']} samples @ {data_info['sampling_rate']}Hz")

# ì¹´íƒˆë¡œê·¸ ë°ì´í„° í™•ì¸
if 'catalog' in locals() and catalog is not None:
    print(f"\nì¹´íƒˆë¡œê·¸ ë°ì´í„°:")
    print(f"  ğŸ“Š {len(catalog)}ê°œ ì§€ì§„ ì´ë²¤íŠ¸")
    print(f"  ğŸ“… ì»¬ëŸ¼: {list(catalog.columns)}")
    if 'magnitude' in catalog.columns:
        print(f"  ğŸ“ˆ ê·œëª¨ ë²”ìœ„: {catalog['magnitude'].min()} - {catalog['magnitude'].max()}")
else:
    print("âš ï¸ ì¹´íƒˆë¡œê·¸ ë°ì´í„° ë¡œë”© í•„ìš”")

    # ì‹¤ì œ ì •ë‹µë°ì´í„° íŒŒì¼ ì½ê¸°
    try:
        import pandas as pd

        # Excel íŒŒì¼ ì½ê¸° (í—¤ë”ëŠ” 2ë²ˆì§¸ í–‰, ë°ì´í„°ëŠ” 4ë²ˆì§¸ í–‰ë¶€í„°)
        raw_catalog = pd.read_excel("inCountryEarthquakeList_20060101_20250704.xlsx",
                                   header=1, skiprows=[2])  # 2ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ, 3ë²ˆì§¸ í–‰ ê±´ë„ˆë›°ê¸°

        # ë°ì´í„° ì •ë¦¬
        # ìœ íš¨í•œ ë°ì´í„°ë§Œ í•„í„°ë§ (numberì™€ origin_timeì´ ìˆëŠ” í–‰)
        valid_mask = raw_catalog['number'].notna() & raw_catalog['origin_time'].notna()
        catalog_clean = raw_catalog[valid_mask].copy()

        # ìœ„ë„/ê²½ë„ ë¬¸ìì—´ ì²˜ë¦¬ ("36.85 N" -> 36.85)
        if 'latitude' in catalog_clean.columns:
            catalog_clean['latitude'] = catalog_clean['latitude'].astype(str).str.replace(' N', '').str.replace(' S', '').astype(float)
        if 'longitude' in catalog_clean.columns:
            catalog_clean['longitude'] = catalog_clean['longitude'].astype(str).str.replace(' E', '').str.replace(' W', '').astype(float)

        # origin_timeì„ datetimeìœ¼ë¡œ ë³€í™˜ (Excel ìˆ«ìë¥¼ ë‚ ì§œë¡œ)
        # Excelì˜ ìˆ«ì ë‚ ì§œë¥¼ pandas datetimeìœ¼ë¡œ ë³€í™˜
        catalog_clean['origin_time'] = pd.to_datetime(catalog_clean['origin_time'], origin='1899-12-30', unit='D')

        # ìµœì¢… ì¹´íƒˆë¡œê·¸
        catalog = catalog_clean.reset_index(drop=True)

        print(f"âœ… ì •ë‹µë°ì´í„° ë¡œë”© ì™„ë£Œ!")
        print(f"  ğŸ“Š ì´ {len(catalog)}ê°œ ì§€ì§„ ì´ë²¤íŠ¸")
        print(f"  ğŸ“… ì»¬ëŸ¼: {list(catalog.columns)}")
        print(f"  ğŸ“ˆ ê·œëª¨ ë²”ìœ„: {catalog['magnitude'].min():.1f} - {catalog['magnitude'].max():.1f}")
        print(f"  ğŸ“ ìœ„ì¹˜ ë²”ìœ„:")
        print(f"    ìœ„ë„: {catalog['latitude'].min():.2f}Â°N - {catalog['latitude'].max():.2f}Â°N")
        print(f"    ê²½ë„: {catalog['longitude'].min():.2f}Â°E - {catalog['longitude'].max():.2f}Â°E")
        print(f"  ğŸ“… ì‹œê°„ ë²”ìœ„: {catalog['origin_time'].min()} ~ {catalog['origin_time'].max()}")

        print(f"\nì²« 3ê°œ ì´ë²¤íŠ¸:")
        print(catalog[['number', 'origin_time', 'magnitude', 'depth', 'latitude', 'longitude', 'location']].head(3))

    except Exception as e:
        print(f"âŒ ì •ë‹µë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}")
        # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        catalog = pd.DataFrame({
            'number': [1, 2],
            'origin_time': pd.to_datetime(['2022-01-01T10:00:00', '2022-01-02T15:30:00']),
            'magnitude': [2.7, 3.5],
            'depth': [10.0, 8.0],
            'latitude': [36.123, 36.789],
            'longitude': [127.456, 128.123],
            'location': ['í…ŒìŠ¤íŠ¸ ì§€ì—­ 1', 'í…ŒìŠ¤íŠ¸ ì§€ì—­ 2']
        })
        print(f"  ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±: {len(catalog)}ê°œ ì´ë²¤íŠ¸")

# ============================================================================
# 2ë‹¨ê³„: 3ì±„ë„ ë°ì´í„° ê²°í•©
# ============================================================================
print(f"\nğŸ”— 2ë‹¨ê³„: 3ì±„ë„ ë°ì´í„° ê²°í•©")

# ì±„ë„ ìˆœì„œ ì •ì˜ (Z, 1, 2 ìˆœì„œë¡œ)
channel_order = ['vertical_BHZ', 'horizontal_1_BH1', 'horizontal_2_BH2']
combined_channels = []

print("ì±„ë„ ê²°í•© ì§„í–‰:")
for channel_name in channel_order:
    if channel_name in final_processed_data:
        data = final_processed_data[channel_name]['data']
        combined_channels.append(data)
        print(f"  âœ… {channel_name}: {len(data)} samples ì¶”ê°€")
    else:
        print(f"  âŒ {channel_name}: ì±„ë„ ì—†ìŒ")

if len(combined_channels) == 3:
    # (ì‹œê°„, ì±„ë„) í˜•íƒœë¡œ ê²°í•©
    combined_data = np.column_stack(combined_channels)
    print(f"âœ… 3ì±„ë„ ê²°í•© ì™„ë£Œ: {combined_data.shape} (ì‹œê°„ x ì±„ë„)")

    # ê¸°ë³¸ í†µê³„
    print(f"  ğŸ“Š ë°ì´í„° ë²”ìœ„: {combined_data.min():.3f} ~ {combined_data.max():.3f}")
    print(f"  ğŸ“Š í‰ê· : {np.mean(combined_data):.3f}")
    print(f"  ğŸ“Š í‘œì¤€í¸ì°¨: {np.std(combined_data):.3f}")
else:
    print("âŒ 3ì±„ë„ ê²°í•© ì‹¤íŒ¨")
    combined_data = None

# ============================================================================
# 3ë‹¨ê³„: ì‹œê°„ ê¸°ì¤€ ìœˆë„ì‰
# ============================================================================
print(f"\nâ° 3ë‹¨ê³„: ì‹œê°„ ê¸°ì¤€ ìœˆë„ì‰")

# ìœˆë„ìš° íŒŒë¼ë¯¸í„° ì„¤ì •
sampling_rate = 40  # Hz
window_duration = 20  # ì´ˆ
window_samples = int(window_duration * sampling_rate)  # 800 samples
overlap_ratio = 0.5  # 50% ê²¹ì¹¨
overlap_samples = int(window_samples * overlap_ratio)

print(f"ìœˆë„ìš° ì„¤ì •:")
print(f"  ğŸ• ìœˆë„ìš° ê¸¸ì´: {window_duration}ì´ˆ ({window_samples} samples)")
print(f"  ğŸ”„ ê²¹ì¹¨: {overlap_ratio*100}% ({overlap_samples} samples)")

# ì¹´íƒˆë¡œê·¸ ê¸°ë°˜ ìœˆë„ì‰ í•¨ìˆ˜
def create_earthquake_windows(combined_data, catalog, sampling_rate,
                             window_samples, before_seconds=10, after_seconds=10):
    """ì§€ì§„ ì´ë²¤íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ìœˆë„ìš° ìƒì„±"""

    windows = []
    labels = []
    metadata = []

    print(f"\nì§€ì§„ ì´ë²¤íŠ¸ë³„ ìœˆë„ìš° ìƒì„±:")

    # pandas DataFrameì„ ì•ˆì „í•˜ê²Œ ìˆœíšŒ
    for idx in range(len(catalog)):
        try:
            # ilocì„ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ í–‰ ì ‘ê·¼
            event = catalog.iloc[idx]
            magnitude = float(event['magnitude'])

            print(f"  ğŸ“ ì´ë²¤íŠ¸ {idx+1}: M{magnitude}")

            # ì‹œê°„ ì •ë³´ (ì‹¤ì œë¡œëŠ” ì¹´íƒˆë¡œê·¸ì˜ ì‹œê°„ê³¼ ì§€ì§„íŒŒ ë°ì´í„°ì˜ ì‹œê°„ì„ ë§¤ì¹­í•´ì•¼ í•¨)
            # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ ë°ì´í„° ì¤‘ì•™ ë¶€ë¶„ì„ ì§€ì§„ ë°œìƒ ì‹œì ìœ¼ë¡œ ê°€ì •
            total_samples = len(combined_data)
            earthquake_sample = total_samples // 2  # ì¤‘ì•™ì ì„ ì§€ì§„ ë°œìƒìœ¼ë¡œ ê°€ì •

            # ì§€ì§„ ì „í›„ êµ¬ê°„ ê³„ì‚°
            before_samples = int(before_seconds * sampling_rate)
            after_samples = int(after_seconds * sampling_rate)

            start_sample = earthquake_sample - before_samples
            end_sample = earthquake_sample + after_samples

            # ìœ íš¨ ë²”ìœ„ í™•ì¸
            if start_sample >= 0 and end_sample <= total_samples:
                event_window = combined_data[start_sample:end_sample]

                # ìœˆë„ìš°ê°€ ì¶©ë¶„íˆ ê¸´ì§€ í™•ì¸
                if len(event_window) >= window_samples:
                    # ì´ë²¤íŠ¸ ìœˆë„ìš° ë‚´ì—ì„œ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„±
                    event_window_count = 0
                    overlap_samples = window_samples // 2  # 50% ê²¹ì¹¨

                    for i in range(0, len(event_window) - window_samples + 1, overlap_samples):
                        window = event_window[i:i + window_samples]

                        if len(window) == window_samples:
                            windows.append(window)

                            # ì•ˆì „í•˜ê²Œ ë¼ë²¨ ìƒì„±
                            label_dict = {
                                'magnitude': magnitude,
                                'depth': float(event['depth']),
                                'latitude': float(event['latitude']),
                                'longitude': float(event['longitude']),
                                'event_id': idx,
                                'window_in_event': event_window_count
                            }
                            labels.append(label_dict)

                            metadata_dict = {
                                'earthquake_sample': earthquake_sample,
                                'window_start': start_sample + i,
                                'window_end': start_sample + i + window_samples,
                                'relative_to_earthquake': i - before_samples
                            }
                            metadata.append(metadata_dict)

                            event_window_count += 1

                    print(f"    âœ… {event_window_count}ê°œ ìœˆë„ìš° ìƒì„±")
                else:
                    print(f"    âš ï¸ ì´ë²¤íŠ¸ ìœˆë„ìš° ë„ˆë¬´ ì§§ìŒ: {len(event_window)} samples")
            else:
                print(f"    âš ï¸ ì´ë²¤íŠ¸ê°€ ë°ì´í„° ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨")

        except Exception as e:
            print(f"    âŒ ì´ë²¤íŠ¸ {idx+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            continue

    return np.array(windows), labels, metadata

# ì§€ì§„ ì´ë²¤íŠ¸ ê¸°ë°˜ ìœˆë„ìš° ìƒì„±
print("ì§€ì§„ ì´ë²¤íŠ¸ ê¸°ë°˜ ìœˆë„ìš° ìƒì„± ì‹œì‘...")

# ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ ì•ˆì „í•˜ê²Œ í™•ì¸
if combined_data is not None:
    print(f"âœ… combined_data ì¤€ë¹„ë¨: {combined_data.shape}")
else:
    print("âŒ combined_data ì—†ìŒ")

try:
    if len(catalog) > 0:
        print(f"âœ… catalog ì¤€ë¹„ë¨: {len(catalog)}ê°œ ì´ë²¤íŠ¸")
        catalog_ready = True
    else:
        print("âŒ catalog ë¹„ì–´ìˆìŒ")
        catalog_ready = False
except:
    print("âŒ catalog ë¬¸ì œ ìˆìŒ")
    catalog_ready = False

# ì‹¤ì œ ìœˆë„ìš° ìƒì„±
if combined_data is not None and catalog_ready:
    print("ìœˆë„ìš° ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ...")
    event_windows, event_labels, event_metadata = create_earthquake_windows(
        combined_data, catalog, sampling_rate, window_samples
    )
else:
    print("âš ï¸ ìœˆë„ìš° ìƒì„± ì¡°ê±´ ë¯¸ì¶©ì¡±")
    event_windows = np.array([])
    event_labels = []
    event_metadata = []

    print(f"\nâœ… ì´ë²¤íŠ¸ ê¸°ë°˜ ìœˆë„ìš° ìƒì„± ì™„ë£Œ:")
    print(f"  ğŸ“Š ì´ ìœˆë„ìš° ìˆ˜: {len(event_windows)}")
    if len(event_windows) > 0:
        print(f"  ğŸ“Š ìœˆë„ìš° shape: {event_windows[0].shape}")
        print(f"  ğŸ“Š ì „ì²´ shape: {event_windows.shape}")

# ============================================================================
# 4ë‹¨ê³„: ë°°ê²½ ë…¸ì´ì¦ˆ ìœˆë„ìš° ìƒì„± (ì§€ì§„ì´ ì—†ëŠ” êµ¬ê°„)
# ============================================================================
print(f"\nğŸŒŠ 4ë‹¨ê³„: ë°°ê²½ ë…¸ì´ì¦ˆ ìœˆë„ìš° ìƒì„±")

def create_background_windows(combined_data, window_samples, overlap_samples,
                             exclude_ranges=None):
    """ë°°ê²½ ë…¸ì´ì¦ˆ ìœˆë„ìš° ìƒì„± (ì§€ì§„ì´ ì—†ëŠ” êµ¬ê°„)"""

    background_windows = []
    background_metadata = []

    # ì „ì²´ ë°ì´í„°ì—ì„œ ì§€ì§„ êµ¬ê°„ì„ ì œì™¸í•œ ë¶€ë¶„ì—ì„œ ìœˆë„ìš° ìƒì„±
    total_samples = len(combined_data)

    # ì§€ì§„ êµ¬ê°„ ì œì™¸ (ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ì¤‘ì•™ 1/3 êµ¬ê°„ì„ ì§€ì§„ êµ¬ê°„ìœ¼ë¡œ ê°€ì •)
    exclude_start = total_samples // 3
    exclude_end = total_samples * 2 // 3

    print(f"ë°°ê²½ ë…¸ì´ì¦ˆ êµ¬ê°„:")
    print(f"  ğŸ”¸ êµ¬ê°„ 1: 0 ~ {exclude_start} samples")
    print(f"  ğŸ”¸ êµ¬ê°„ 2: {exclude_end} ~ {total_samples} samples")

    # ì²« ë²ˆì§¸ êµ¬ê°„ì—ì„œ ìœˆë„ìš° ìƒì„±
    for i in range(0, exclude_start - window_samples, overlap_samples):
        window = combined_data[i:i + window_samples]
        if len(window) == window_samples:
            background_windows.append(window)
            background_metadata.append({
                'window_start': i,
                'window_end': i + window_samples,
                'type': 'background_1'
            })

    # ë‘ ë²ˆì§¸ êµ¬ê°„ì—ì„œ ìœˆë„ìš° ìƒì„±
    for i in range(exclude_end, total_samples - window_samples, overlap_samples):
        window = combined_data[i:i + window_samples]
        if len(window) == window_samples:
            background_windows.append(window)
            background_metadata.append({
                'window_start': i,
                'window_end': i + window_samples,
                'type': 'background_2'
            })

    return np.array(background_windows), background_metadata

# ë°°ê²½ ë…¸ì´ì¦ˆ ìœˆë„ìš° ìƒì„±
if combined_data is not None:
    background_windows, background_metadata = create_background_windows(
        combined_data, window_samples, overlap_samples
    )

    print(f"âœ… ë°°ê²½ ë…¸ì´ì¦ˆ ìœˆë„ìš° ìƒì„± ì™„ë£Œ:")
    print(f"  ğŸ“Š ë°°ê²½ ìœˆë„ìš° ìˆ˜: {len(background_windows)}")
    if len(background_windows) > 0:
        print(f"  ğŸ“Š ìœˆë„ìš° shape: {background_windows[0].shape}")

# ============================================================================
# 5ë‹¨ê³„: ë…¸ì´ì¦ˆ ì œê±°ìš© ë°ì´í„° ìŒ ìƒì„±
# ============================================================================
print(f"\nğŸ¯ 5ë‹¨ê³„: ë…¸ì´ì¦ˆ ì œê±°ìš© ë°ì´í„° ìŒ ìƒì„±")

def add_realistic_noise(clean_windows, noise_level=0.1):
    """í˜„ì‹¤ì ì¸ ë…¸ì´ì¦ˆë¥¼ clean ì‹ í˜¸ì— ì¶”ê°€"""

    noisy_windows = []

    print(f"ë…¸ì´ì¦ˆ ì¶”ê°€ ì§„í–‰:")
    print(f"  ğŸ”Š ë…¸ì´ì¦ˆ ë ˆë²¨: {noise_level}")

    for i, clean_window in enumerate(clean_windows):
        # 1. ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ
        gaussian_noise = np.random.normal(0, noise_level * np.std(clean_window),
                                        clean_window.shape)

        # 2. ì „ë ¥ì„  ê°„ì„­ (60Hz)
        time_axis = np.arange(len(clean_window)) / sampling_rate
        power_line_noise = 0.05 * noise_level * np.sin(2 * np.pi * 60 * time_axis)

        # 3. ì €ì£¼íŒŒ ë“œë¦¬í”„íŠ¸
        drift_noise = 0.02 * noise_level * np.sin(2 * np.pi * 0.1 * time_axis)

        # ê° ì±„ë„ì— ë‹¤ë¥¸ ë…¸ì´ì¦ˆ íŠ¹ì„± ì ìš©
        noisy_window = clean_window.copy()
        for ch in range(clean_window.shape[1]):
            channel_noise = (gaussian_noise[:, ch] +
                           power_line_noise * (0.5 + 0.5 * ch) +  # ì±„ë„ë³„ ë‹¤ë¥¸ ê°•ë„
                           drift_noise[:, ch] if len(drift_noise.shape) > 1
                           else np.broadcast_to(drift_noise, (len(drift_noise),)))
            noisy_window[:, ch] += channel_noise

        noisy_windows.append(noisy_window)

        if (i + 1) % 10 == 0 or i == len(clean_windows) - 1:
            print(f"    ì§„í–‰ë¥ : {i+1}/{len(clean_windows)} ({(i+1)/len(clean_windows)*100:.1f}%)")

    return np.array(noisy_windows)

# Clean ë°ì´í„°ë¡œ ì´ë²¤íŠ¸ ìœˆë„ìš° ì‚¬ìš©
if 'event_windows' in locals() and len(event_windows) > 0:
    clean_data = event_windows
    noisy_data = add_realistic_noise(clean_data, noise_level=0.15)

    print(f"âœ… ë…¸ì´ì¦ˆ ì œê±°ìš© ë°ì´í„° ìŒ ìƒì„± ì™„ë£Œ:")
    print(f"  ğŸ“Š Clean ë°ì´í„°: {clean_data.shape}")
    print(f"  ğŸ“Š Noisy ë°ì´í„°: {noisy_data.shape}")

    # ë…¸ì´ì¦ˆ ì¶”ê°€ íš¨ê³¼ í™•ì¸
    print(f"  ğŸ“ˆ ë…¸ì´ì¦ˆ ì¶”ê°€ íš¨ê³¼:")
    print(f"    Clean í‘œì¤€í¸ì°¨: {np.std(clean_data):.4f}")
    print(f"    Noisy í‘œì¤€í¸ì°¨: {np.std(noisy_data):.4f}")
    print(f"    ë…¸ì´ì¦ˆ ë¹„ìœ¨: {(np.std(noisy_data) - np.std(clean_data))/np.std(clean_data)*100:.1f}%")

# ============================================================================
# 6ë‹¨ê³„: ìµœì¢… ì •ê·œí™” ë° ë°ì´í„°ì…‹ êµ¬ì„±
# ============================================================================
print(f"\nğŸ“ 6ë‹¨ê³„: ìµœì¢… ì •ê·œí™” ë° ë°ì´í„°ì…‹ êµ¬ì„±")

def normalize_data(data, method='z_score'):
    """ë°ì´í„° ì •ê·œí™”"""
    if method == 'z_score':
        mean = np.mean(data)
        std = np.std(data)
        normalized = (data - mean) / (std + 1e-8)  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
    elif method == 'min_max':
        min_val = np.min(data)
        max_val = np.max(data)
        normalized = (data - min_val) / (max_val - min_val + 1e-8)

    return normalized, {'mean': mean if method == 'z_score' else min_val,
                       'scale': std if method == 'z_score' else (max_val - min_val)}

# ë°ì´í„° ì •ê·œí™”
if 'noisy_data' in locals() and 'clean_data' in locals():
    print("ë°ì´í„° ì •ê·œí™” ì§„í–‰:")

    # ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•´ í†µê³„ ê³„ì‚°
    all_data = np.concatenate([noisy_data.flatten(), clean_data.flatten()])

    # Z-score ì •ê·œí™”
    normalized_noisy, norm_stats = normalize_data(noisy_data, method='z_score')
    normalized_clean, _ = normalize_data(clean_data, method='z_score')

    print(f"  âœ… Z-score ì •ê·œí™” ì™„ë£Œ")
    print(f"    í‰ê· : {norm_stats['mean']:.6f}")
    print(f"    í‘œì¤€í¸ì°¨: {norm_stats['scale']:.6f}")
    print(f"    ì •ê·œí™” í›„ ë²”ìœ„: {normalized_noisy.min():.3f} ~ {normalized_noisy.max():.3f}")

# ============================================================================
# 7ë‹¨ê³„: í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• 
# ============================================================================
print(f"\nğŸ“š 7ë‹¨ê³„: í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• ")

def split_dataset(X, y, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):
    """ë°ì´í„°ì…‹ì„ train/validation/testë¡œ ë¶„í• """

    total_samples = len(X)

    # ì¸ë±ìŠ¤ ì„ê¸°
    indices = np.random.permutation(total_samples)

    # ë¶„í•  ì§€ì  ê³„ì‚°
    train_end = int(total_samples * train_ratio)
    val_end = int(total_samples * (train_ratio + val_ratio))

    # ë¶„í• 
    train_idx = indices[:train_end]
    val_idx = indices[train_end:val_end]
    test_idx = indices[val_end:]

    return {
        'train': {'X': X[train_idx], 'y': y[train_idx], 'indices': train_idx},
        'val': {'X': X[val_idx], 'y': y[val_idx], 'indices': val_idx},
        'test': {'X': X[test_idx], 'y': y[test_idx], 'indices': test_idx}
    }

# ë°ì´í„° ë¶„í• 
if 'normalized_noisy' in locals() and 'normalized_clean' in locals():
    # ì‹œë“œ ì„¤ì •ìœ¼ë¡œ ì¬í˜„ ê°€ëŠ¥í•œ ë¶„í• 
    np.random.seed(42)

    dataset = split_dataset(normalized_noisy, normalized_clean,
                          train_ratio=0.7, val_ratio=0.2, test_ratio=0.1)

    print(f"âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
    for split_name, split_data in dataset.items():
        print(f"  ğŸ“Š {split_name.upper()}: {len(split_data['X'])}ê°œ ìƒ˜í”Œ")
        print(f"    X shape: {split_data['X'].shape}")
        print(f"    y shape: {split_data['y'].shape}")

# ============================================================================
# ìµœì¢… ê²°ê³¼ ìš”ì•½
# ============================================================================
print(f"\nğŸ‰ ë”¥ëŸ¬ë‹ìš© ì „ì²˜ë¦¬ ì™„ë£Œ!")
print("="*60)

if 'dataset' in locals():
    print(f"ğŸ“Š ìµœì¢… ë°ì´í„°ì…‹:")
    print(f"  ğŸ¯ ì‘ì—…: ì§€ì§„íŒŒ ë…¸ì´ì¦ˆ ì œê±° (Denoising)")
    print(f"  ğŸ“ ì…ë ¥ ì°¨ì›: {dataset['train']['X'].shape[1:]} (ì‹œê°„ x ì±„ë„)")
    print(f"  ğŸ“ˆ ë°ì´í„° ì •ê·œí™”: Z-score")
    print(f"  ğŸ”€ ë°ì´í„° ë¶„í• :")
    for split_name, split_data in dataset.items():
        ratio = len(split_data['X']) / (len(dataset['train']['X']) +
                                      len(dataset['val']['X']) +
                                      len(dataset['test']['X'])) * 100
        print(f"    - {split_name.upper()}: {len(split_data['X'])}ê°œ ({ratio:.1f}%)")

    print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„: ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì¤€ë¹„ ì™„ë£Œ!")
    print(f"  ğŸ’¡ ì¶”ì²œ ëª¨ë¸: U-Net, Autoencoder, or Transformer-based denoiser")
    print(f"  ğŸ“ ì‚¬ìš©ë²•:")
    print(f"    X_train = dataset['train']['X']")
    print(f"    y_train = dataset['train']['y']")
    print(f"    # ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
else:
    print(f"âŒ ì¼ë¶€ ë‹¨ê³„ ì‹¤íŒ¨ - ë””ë²„ê¹… í•„ìš”")

print(f"\nâœ… ë”¥ëŸ¬ë‹ìš© ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! ğŸŠ")



print("ğŸ”„ ìƒˆë¡œìš´ ì •ë‹µë°ì´í„° ê°•ì œ ë¡œë”© ì‹œì‘!")

# 1. ê¸°ì¡´ catalog ë³€ìˆ˜ ì™„ì „íˆ ì‚­ì œ
if 'catalog' in locals():
    del catalog
    print("âœ… ê¸°ì¡´ catalog ë³€ìˆ˜ ì‚­ì œ")

# 2. ìƒˆë¡œìš´ Excel íŒŒì¼ ê°•ì œ ë¡œë”©
import pandas as pd

try:
    print("ğŸ“‚ Excel íŒŒì¼ ì½ê¸° ì¤‘...")

    # Excel íŒŒì¼ ì½ê¸° (ì •í™•í•œ êµ¬ì¡°ë¡œ)
    raw_data = pd.read_excel("inCountryEarthquakeList_20060101_20250704.xlsx",
                            header=1,      # 2ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ
                            skiprows=[2])  # 3ë²ˆì§¸ í–‰ ê±´ë„ˆë›°ê¸°

    print(f"ğŸ“Š ì›ì‹œ ë°ì´í„° ë¡œë”©: {len(raw_data)} í–‰")

    # 3. ìœ íš¨í•œ ë°ì´í„°ë§Œ í•„í„°ë§
    print("ğŸ§¹ ë°ì´í„° ì •ë¦¬ ì¤‘...")

    # numberì™€ origin_timeì´ ìˆëŠ” í–‰ë§Œ ìœ ì§€
    valid_mask = raw_data['number'].notna() & raw_data['origin_time'].notna()
    catalog_clean = raw_data[valid_mask].copy()

    print(f"ğŸ“Š ìœ íš¨ ë°ì´í„°: {len(catalog_clean)} í–‰")

    # 4. ë°ì´í„° íƒ€ì… ë³€í™˜
    print("ğŸ”§ ë°ì´í„° ë³€í™˜ ì¤‘...")

    # ìœ„ë„/ê²½ë„ ë¬¸ìì—´ ì²˜ë¦¬ ("36.85 N" â†’ 36.85)
    catalog_clean['latitude'] = catalog_clean['latitude'].astype(str).str.replace(' N', '').str.replace(' S', '').astype(float)
    catalog_clean['longitude'] = catalog_clean['longitude'].astype(str).str.replace(' E', '').str.replace(' W', '').astype(float)

    # Excel ë‚ ì§œ ìˆ«ìë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
    catalog_clean['origin_time'] = pd.to_datetime(catalog_clean['origin_time'], origin='1899-12-30', unit='D')

    # 5. ìµœì¢… catalog ìƒì„±
    catalog = catalog_clean.reset_index(drop=True)

    print(f"ğŸ‰ ìƒˆë¡œìš´ ì •ë‹µë°ì´í„° ë¡œë”© ì™„ë£Œ!")
    print(f"  ğŸ“Š ì´ ì´ë²¤íŠ¸: {len(catalog)}ê°œ")
    print(f"  ğŸ“ˆ ê·œëª¨ ë²”ìœ„: {catalog['magnitude'].min():.1f} - {catalog['magnitude'].max():.1f}")
    print(f"  ğŸ“… ì‹œê°„ ë²”ìœ„: {catalog['origin_time'].min().date()} ~ {catalog['origin_time'].max().date()}")
    print(f"  ğŸŒ ìœ„ì¹˜ ë²”ìœ„:")
    print(f"    ìœ„ë„: {catalog['latitude'].min():.1f}Â°N - {catalog['latitude'].max():.1f}Â°N")
    print(f"    ê²½ë„: {catalog['longitude'].min():.1f}Â°E - {catalog['longitude'].max():.1f}Â°E")

    print(f"\nğŸ“‹ ì²« 3ê°œ ì´ë²¤íŠ¸:")
    display_cols = ['number', 'origin_time', 'magnitude', 'depth', 'latitude', 'longitude']
    print(catalog[display_cols].head(3))

    print(f"\nâœ… ì„±ê³µ! ì´ì œ {len(catalog)}ê°œ ì´ë²¤íŠ¸ë¡œ ì „ì²˜ë¦¬ ê°€ëŠ¥!")

except FileNotFoundError:
    print("âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
    print("ğŸ“‚ í˜„ì¬ ë””ë ‰í† ë¦¬ íŒŒì¼ í™•ì¸:")
    import os
    for f in os.listdir('.'):
        if 'earthquake' in f.lower() or f.endswith('.xlsx'):
            print(f"  - {f}")

except Exception as e:
    print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    print("ğŸ’¡ ë‹¤ë¥¸ ë°©ë²• ì‹œë„:")
    print("  1. íŒŒì¼ëª… í™•ì¸")
    print("  2. Excel ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜: pip install openpyxl")
    print("  3. ì»¤ë„ ì¬ì‹œì‘ í›„ ë‹¤ì‹œ ì‹œë„")



print("ğŸ” CSV íŒŒì¼ ì°¾ê¸°")

import os

# í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  CSV íŒŒì¼ í™•ì¸
print("ğŸ“‚ í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ CSV íŒŒì¼ë“¤:")
csv_files = []
for f in os.listdir('.'):
    if f.endswith('.csv') and 'earthquake' in f.lower():
        csv_files.append(f)
        print(f"  âœ… {f}")

if csv_files:
    # ê°€ì¥ ì í•©í•œ CSV íŒŒì¼ ì„ íƒ
    target_csv = csv_files[0]  # ì²« ë²ˆì§¸ íŒŒì¼ ì‚¬ìš©
    print(f"\nğŸ“„ ì‚¬ìš©í•  CSV íŒŒì¼: {target_csv}")

    # CSV íŒŒì¼ ì½ê¸°
    import pandas as pd
    try:
        catalog = pd.read_csv(target_csv, skiprows=2)
        print(f"âœ… CSV ì½ê¸° ì„±ê³µ: {len(catalog)} í–‰")

        # ì²« ëª‡ í–‰ í™•ì¸
        print("\nğŸ“‹ ì²« 5í–‰:")
        print(catalog.head())

        # ìœ íš¨í•œ ë°ì´í„° ê°œìˆ˜ í™•ì¸
        valid_count = len(catalog.dropna(subset=[catalog.columns[0]]))
        print(f"ğŸ“Š ìœ íš¨í•œ ë°ì´í„°: {valid_count}ê°œ")

    except Exception as e:
        print(f"âŒ CSV ì½ê¸° ì‹¤íŒ¨: {e}")
else:
    print("âŒ ì§€ì§„ ê´€ë ¨ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    print("ğŸ’¡ íŒŒì¼ëª…ì„ ì§ì ‘ í™•ì¸í•´ì£¼ì„¸ìš”")



    print("ğŸ”§ êµ¬ê¸€ ì½”ë©ìš© ì•ˆì „í•œ CSV ì½ê¸°")

def safe_read_earthquake_csv(filename):
    """êµ¬ê¸€ ì½”ë©ì—ì„œ ì•ˆì „í•˜ê²Œ CSV ì½ê¸°"""

    try:
        # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì‹œë„
        methods = [
            {"skiprows": 2, "encoding": "utf-8"},
            {"header": 1, "skiprows": [2], "encoding": "utf-8"},
            {"skiprows": 2, "encoding": "cp949"},
            {"skiprows": 2, "encoding": "euc-kr"},
            {"header": 0, "encoding": "utf-8"},
        ]

        for i, method in enumerate(methods, 1):
            try:
                print(f"  ì‹œë„ {i}: {method}")
                df = pd.read_csv(filename, **method)

                # ìœ íš¨í•œ ë°ì´í„°ì¸ì§€ í™•ì¸
                if len(df) > 100:  # 1000ê°œ ì´ìƒì´ë©´ ì„±ê³µ
                    print(f"    âœ… ì„±ê³µ: {len(df)} í–‰")

                    # ì»¬ëŸ¼ëª… ìˆ˜ì •
                    if len(df.columns) >= 7:
                        df.columns = ['number', 'origin_time_str', 'magnitude', 'depth',
                                    'max_intensity', 'latitude_str', 'longitude_str'] + list(df.columns[7:])

                        # ë°ì´í„° ë³€í™˜
                        df['magnitude'] = pd.to_numeric(df['magnitude'], errors='coerce')
                        df['latitude'] = df['latitude_str'].astype(str).str.extract(r'([\d.]+)').astype(float)
                        df['longitude'] = df['longitude_str'].astype(str).str.extract(r'([\d.]+)').astype(float)

                        try:
                            df['origin_time'] = pd.to_datetime(df['origin_time_str'], errors='coerce')
                        except:
                            df['origin_time'] = pd.to_datetime('2022-01-01')

                        # ìœ íš¨í•œ ë°ì´í„°ë§Œ í•„í„°ë§
                        df_clean = df.dropna(subset=['magnitude', 'latitude', 'longitude']).reset_index(drop=True)

                        print(f"    ğŸ“Š ì •ë¦¬ í›„: {len(df_clean)}ê°œ ìœ íš¨í•œ ì´ë²¤íŠ¸")
                        print(f"    ğŸ“ˆ ê·œëª¨ ë²”ìœ„: {df_clean['magnitude'].min():.1f} - {df_clean['magnitude'].max():.1f}")

                        return df_clean

                else:
                    print(f"    âŒ ë°ì´í„° ë¶€ì¡±: {len(df)} í–‰")

            except Exception as e:
                print(f"    âŒ ì‹¤íŒ¨: {str(e)[:50]}...")
                continue

        print("âŒ ëª¨ë“  ë°©ë²• ì‹¤íŒ¨")
        return None

    except Exception as e:
        print(f"âŒ ì „ì²´ ì‹¤íŒ¨: {e}")
        return None

# ì‹¤í–‰
catalog = safe_read_earthquake_csv("inCountryEarthquakeList_2006-01-01_2025-07-04.csv")

if catalog is not None:
    print(f"\nğŸ‰ ì„±ê³µ! {len(catalog)}ê°œ ì§€ì§„ ì´ë²¤íŠ¸ ë¡œë”© ì™„ë£Œ")
else:
    print(f"\nâŒ CSV ì½ê¸° ì‹¤íŒ¨ - ê¸°ë³¸ ë°ì´í„° ì‚¬ìš©")
    # ê¸°ë³¸ ë°ì´í„° ìƒì„±
    catalog = pd.DataFrame({
        'number': [1, 2],
        'origin_time': pd.to_datetime(['2022-01-01T10:00:00', '2022-01-02T15:30:00']),
        'magnitude': [2.7, 3.5],
        'depth': [10.0, 8.0],
        'latitude': [36.123, 36.789],
        'longitude': [127.456, 128.123]
    })



import numpy as np
import pandas as pd
from datetime import datetime, timedelta

print("ğŸš€ ë”¥ëŸ¬ë‹ìš© ì§€ì§„íŒŒ ì „ì²˜ë¦¬ ì‹œì‘!")
print("="*60)

# ============================================================================
# 1ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ ë° ê²€ì¦
# ============================================================================
print("\nğŸ“‹ 1ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ ë° ê²€ì¦")

# ì „ì²˜ë¦¬ëœ ì§€ì§„íŒŒ ë°ì´í„° í™•ì¸
print("ì „ì²˜ë¦¬ëœ ì§€ì§„íŒŒ ë°ì´í„°:")
for channel_name, data_info in final_processed_data.items():
    print(f"  ğŸ”¸ {channel_name}: {data_info['length']} samples @ {data_info['sampling_rate']}Hz")

# ì¹´íƒˆë¡œê·¸ ë°ì´í„° í™•ì¸
if 'catalog' in locals() and catalog is not None:
    print(f"\nì¹´íƒˆë¡œê·¸ ë°ì´í„°:")
    print(f"  ğŸ“Š {len(catalog)}ê°œ ì§€ì§„ ì´ë²¤íŠ¸")
    print(f"  ğŸ“… ì»¬ëŸ¼: {list(catalog.columns)}")
    if 'magnitude' in catalog.columns:
        print(f"  ğŸ“ˆ ê·œëª¨ ë²”ìœ„: {catalog['magnitude'].min()} - {catalog['magnitude'].max()}")
else:
    print("âš ï¸ ì¹´íƒˆë¡œê·¸ ë°ì´í„° ë¡œë”© í•„ìš”")

    # ì‹¤ì œ ì •ë‹µë°ì´í„° íŒŒì¼ ì½ê¸°
    try:
        import pandas as pd

        # Excel íŒŒì¼ ì½ê¸° (í—¤ë”ëŠ” 2ë²ˆì§¸ í–‰, ë°ì´í„°ëŠ” 4ë²ˆì§¸ í–‰ë¶€í„°)
        raw_catalog = pd.read_excel("inCountryEarthquakeList_20060101_20250704.xlsx",
                                   header=1, skiprows=[2])  # 2ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ, 3ë²ˆì§¸ í–‰ ê±´ë„ˆë›°ê¸°

        # ë°ì´í„° ì •ë¦¬
        # ìœ íš¨í•œ ë°ì´í„°ë§Œ í•„í„°ë§ (numberì™€ origin_timeì´ ìˆëŠ” í–‰)
        valid_mask = raw_catalog['number'].notna() & raw_catalog['origin_time'].notna()
        catalog_clean = raw_catalog[valid_mask].copy()

        # ìœ„ë„/ê²½ë„ ë¬¸ìì—´ ì²˜ë¦¬ ("36.85 N" -> 36.85)
        if 'latitude' in catalog_clean.columns:
            catalog_clean['latitude'] = catalog_clean['latitude'].astype(str).str.replace(' N', '').str.replace(' S', '').astype(float)
        if 'longitude' in catalog_clean.columns:
            catalog_clean['longitude'] = catalog_clean['longitude'].astype(str).str.replace(' E', '').str.replace(' W', '').astype(float)

        # origin_timeì„ datetimeìœ¼ë¡œ ë³€í™˜ (Excel ìˆ«ìë¥¼ ë‚ ì§œë¡œ)
        # Excelì˜ ìˆ«ì ë‚ ì§œë¥¼ pandas datetimeìœ¼ë¡œ ë³€í™˜
        catalog_clean['origin_time'] = pd.to_datetime(catalog_clean['origin_time'], origin='1899-12-30', unit='D')

        # ìµœì¢… ì¹´íƒˆë¡œê·¸
        catalog = catalog_clean.reset_index(drop=True)

        print(f"âœ… ì •ë‹µë°ì´í„° ë¡œë”© ì™„ë£Œ!")
        print(f"  ğŸ“Š ì´ {len(catalog)}ê°œ ì§€ì§„ ì´ë²¤íŠ¸")
        print(f"  ğŸ“… ì»¬ëŸ¼: {list(catalog.columns)}")
        print(f"  ğŸ“ˆ ê·œëª¨ ë²”ìœ„: {catalog['magnitude'].min():.1f} - {catalog['magnitude'].max():.1f}")
        print(f"  ğŸ“ ìœ„ì¹˜ ë²”ìœ„:")
        print(f"    ìœ„ë„: {catalog['latitude'].min():.2f}Â°N - {catalog['latitude'].max():.2f}Â°N")
        print(f"    ê²½ë„: {catalog['longitude'].min():.2f}Â°E - {catalog['longitude'].max():.2f}Â°E")
        print(f"  ğŸ“… ì‹œê°„ ë²”ìœ„: {catalog['origin_time'].min()} ~ {catalog['origin_time'].max()}")

        print(f"\nì²« 3ê°œ ì´ë²¤íŠ¸:")
        print(catalog[['number', 'origin_time', 'magnitude', 'depth', 'latitude', 'longitude', 'location']].head(3))

    except Exception as e:
        print(f"âŒ ì •ë‹µë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}")
        # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        catalog = pd.DataFrame({
            'number': [1, 2],
            'origin_time': pd.to_datetime(['2022-01-01T10:00:00', '2022-01-02T15:30:00']),
            'magnitude': [2.7, 3.5],
            'depth': [10.0, 8.0],
            'latitude': [36.123, 36.789],
            'longitude': [127.456, 128.123],
            'location': ['í…ŒìŠ¤íŠ¸ ì§€ì—­ 1', 'í…ŒìŠ¤íŠ¸ ì§€ì—­ 2']
        })
        print(f"  ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±: {len(catalog)}ê°œ ì´ë²¤íŠ¸")

# ============================================================================
# 2ë‹¨ê³„: 3ì±„ë„ ë°ì´í„° ê²°í•©
# ============================================================================
print(f"\nğŸ”— 2ë‹¨ê³„: 3ì±„ë„ ë°ì´í„° ê²°í•©")

# ì»¬ëŸ¼ ê²€ì¦ ë° ìˆ˜ì • (1,664ê°œ ë°ì´í„°ì¸ ê²½ìš°)
if 'catalog' in locals() and len(catalog) > 100:
    print("ğŸ” ëŒ€ìš©ëŸ‰ ì¹´íƒˆë¡œê·¸ ë°ì´í„° ì»¬ëŸ¼ ê²€ì¦...")

    if 'magnitude' not in catalog.columns:
        print("âš ï¸ magnitude ì»¬ëŸ¼ ì—†ìŒ - ì»¬ëŸ¼ êµ¬ì¡° ë¬¸ì œ í•´ê²° ì¤‘...")
        print(f"í˜„ì¬ ì»¬ëŸ¼: {list(catalog.columns)}")

        # ì»¬ëŸ¼ì´ 'Unnamed'ë¡œ ë˜ì–´ìˆë‹¤ë©´ ìˆ˜ì •
        if any('Unnamed' in str(col) for col in catalog.columns):
            if len(catalog.columns) >= 7:
                catalog.columns = ['number', 'origin_time_str', 'magnitude', 'depth', 'max_intensity', 'latitude_str', 'longitude_str'] + list(catalog.columns[7:])

                # ë°ì´í„° ë³€í™˜
                catalog['magnitude'] = pd.to_numeric(catalog['magnitude'], errors='coerce')
                catalog['depth'] = pd.to_numeric(catalog['depth'], errors='coerce')
                catalog['latitude'] = catalog['latitude_str'].astype(str).str.extract(r'([\d.]+)').astype(float)
                catalog['longitude'] = catalog['longitude_str'].astype(str).str.extract(r'([\d.]+)').astype(float)

                try:
                    catalog['origin_time'] = pd.to_datetime(catalog['origin_time_str'], errors='coerce')
                except:
                    catalog['origin_time'] = pd.to_datetime('2022-01-01')  # ê¸°ë³¸ê°’

                # ìœ íš¨í•œ ë°ì´í„°ë§Œ í•„í„°ë§
                catalog = catalog.dropna(subset=['magnitude', 'latitude', 'longitude']).reset_index(drop=True)

                print(f"âœ… ì»¬ëŸ¼ êµ¬ì¡° ìˆ˜ì • ì™„ë£Œ!")
                print(f"  ğŸ“Š ì²˜ë¦¬ëœ ì´ë²¤íŠ¸: {len(catalog)}ê°œ")
                print(f"  ğŸ“ˆ ê·œëª¨ ë²”ìœ„: {catalog['magnitude'].min():.1f} - {catalog['magnitude'].max():.1f}")
    else:
        print("âœ… ì¹´íƒˆë¡œê·¸ ì»¬ëŸ¼ êµ¬ì¡° ì •ìƒ")

# ì±„ë„ ìˆœì„œ ì •ì˜ (Z, 1, 2 ìˆœì„œë¡œ)
channel_order = ['vertical_BHZ', 'horizontal_1_BH1', 'horizontal_2_BH2']
combined_channels = []

print("ì±„ë„ ê²°í•© ì§„í–‰:")
for channel_name in channel_order:
    if channel_name in final_processed_data:
        data = final_processed_data[channel_name]['data']
        combined_channels.append(data)
        print(f"  âœ… {channel_name}: {len(data)} samples ì¶”ê°€")
    else:
        print(f"  âŒ {channel_name}: ì±„ë„ ì—†ìŒ")

if len(combined_channels) == 3:
    # (ì‹œê°„, ì±„ë„) í˜•íƒœë¡œ ê²°í•©
    combined_data = np.column_stack(combined_channels)
    print(f"âœ… 3ì±„ë„ ê²°í•© ì™„ë£Œ: {combined_data.shape} (ì‹œê°„ x ì±„ë„)")

    # ê¸°ë³¸ í†µê³„
    print(f"  ğŸ“Š ë°ì´í„° ë²”ìœ„: {combined_data.min():.3f} ~ {combined_data.max():.3f}")
    print(f"  ğŸ“Š í‰ê· : {np.mean(combined_data):.3f}")
    print(f"  ğŸ“Š í‘œì¤€í¸ì°¨: {np.std(combined_data):.3f}")
else:
    print("âŒ 3ì±„ë„ ê²°í•© ì‹¤íŒ¨")
    combined_data = None

# ============================================================================
# 3ë‹¨ê³„: ì‹œê°„ ê¸°ì¤€ ìœˆë„ì‰
# ============================================================================
print(f"\nâ° 3ë‹¨ê³„: ì‹œê°„ ê¸°ì¤€ ìœˆë„ì‰")

# ìœˆë„ìš° íŒŒë¼ë¯¸í„° ì„¤ì •
sampling_rate = 40  # Hz
window_duration = 20  # ì´ˆ
window_samples = int(window_duration * sampling_rate)  # 800 samples
overlap_ratio = 0.5  # 50% ê²¹ì¹¨
overlap_samples = int(window_samples * overlap_ratio)

print(f"ìœˆë„ìš° ì„¤ì •:")
print(f"  ğŸ• ìœˆë„ìš° ê¸¸ì´: {window_duration}ì´ˆ ({window_samples} samples)")
print(f"  ğŸ”„ ê²¹ì¹¨: {overlap_ratio*100}% ({overlap_samples} samples)")

# ì¹´íƒˆë¡œê·¸ ê¸°ë°˜ ìœˆë„ì‰ í•¨ìˆ˜
def create_earthquake_windows(combined_data, catalog, sampling_rate,
                             window_samples, before_seconds=10, after_seconds=10):
    """ì§€ì§„ ì´ë²¤íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ìœˆë„ìš° ìƒì„±"""

    windows = []
    labels = []
    metadata = []

    print(f"\nì§€ì§„ ì´ë²¤íŠ¸ë³„ ìœˆë„ìš° ìƒì„±:")

    # pandas DataFrameì„ ì•ˆì „í•˜ê²Œ ìˆœíšŒ
    for idx in range(len(catalog)):
        try:
            # ilocì„ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ í–‰ ì ‘ê·¼
            event = catalog.iloc[idx]
            magnitude = float(event['magnitude'])

            print(f"  ğŸ“ ì´ë²¤íŠ¸ {idx+1}: M{magnitude}")

            # ì‹œê°„ ì •ë³´ (ì‹¤ì œë¡œëŠ” ì¹´íƒˆë¡œê·¸ì˜ ì‹œê°„ê³¼ ì§€ì§„íŒŒ ë°ì´í„°ì˜ ì‹œê°„ì„ ë§¤ì¹­í•´ì•¼ í•¨)
            # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ ë°ì´í„° ì¤‘ì•™ ë¶€ë¶„ì„ ì§€ì§„ ë°œìƒ ì‹œì ìœ¼ë¡œ ê°€ì •
            total_samples = len(combined_data)
            earthquake_sample = total_samples // 2  # ì¤‘ì•™ì ì„ ì§€ì§„ ë°œìƒìœ¼ë¡œ ê°€ì •

            # ì§€ì§„ ì „í›„ êµ¬ê°„ ê³„ì‚°
            before_samples = int(before_seconds * sampling_rate)
            after_samples = int(after_seconds * sampling_rate)

            start_sample = earthquake_sample - before_samples
            end_sample = earthquake_sample + after_samples

            # ìœ íš¨ ë²”ìœ„ í™•ì¸
            if start_sample >= 0 and end_sample <= total_samples:
                event_window = combined_data[start_sample:end_sample]

                # ìœˆë„ìš°ê°€ ì¶©ë¶„íˆ ê¸´ì§€ í™•ì¸
                if len(event_window) >= window_samples:
                    # ì´ë²¤íŠ¸ ìœˆë„ìš° ë‚´ì—ì„œ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„±
                    event_window_count = 0
                    overlap_samples = window_samples // 2  # 50% ê²¹ì¹¨

                    for i in range(0, len(event_window) - window_samples + 1, overlap_samples):
                        window = event_window[i:i + window_samples]

                        if len(window) == window_samples:
                            windows.append(window)

                            # ì•ˆì „í•˜ê²Œ ë¼ë²¨ ìƒì„±
                            label_dict = {
                                'magnitude': magnitude,
                                'depth': float(event['depth']),
                                'latitude': float(event['latitude']),
                                'longitude': float(event['longitude']),
                                'event_id': idx,
                                'window_in_event': event_window_count
                            }
                            labels.append(label_dict)

                            metadata_dict = {
                                'earthquake_sample': earthquake_sample,
                                'window_start': start_sample + i,
                                'window_end': start_sample + i + window_samples,
                                'relative_to_earthquake': i - before_samples
                            }
                            metadata.append(metadata_dict)

                            event_window_count += 1

                    print(f"    âœ… {event_window_count}ê°œ ìœˆë„ìš° ìƒì„±")
                else:
                    print(f"    âš ï¸ ì´ë²¤íŠ¸ ìœˆë„ìš° ë„ˆë¬´ ì§§ìŒ: {len(event_window)} samples")
            else:
                print(f"    âš ï¸ ì´ë²¤íŠ¸ê°€ ë°ì´í„° ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨")

        except Exception as e:
            print(f"    âŒ ì´ë²¤íŠ¸ {idx+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            continue

    return np.array(windows), labels, metadata

# ì§€ì§„ ì´ë²¤íŠ¸ ê¸°ë°˜ ìœˆë„ìš° ìƒì„±
print("ì§€ì§„ ì´ë²¤íŠ¸ ê¸°ë°˜ ìœˆë„ìš° ìƒì„± ì‹œì‘...")

# ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ ì•ˆì „í•˜ê²Œ í™•ì¸
if combined_data is not None:
    print(f"âœ… combined_data ì¤€ë¹„ë¨: {combined_data.shape}")
else:
    print("âŒ combined_data ì—†ìŒ")

try:
    if len(catalog) > 0:
        print(f"âœ… catalog ì¤€ë¹„ë¨: {len(catalog)}ê°œ ì´ë²¤íŠ¸")
        catalog_ready = True
    else:
        print("âŒ catalog ë¹„ì–´ìˆìŒ")
        catalog_ready = False
except:
    print("âŒ catalog ë¬¸ì œ ìˆìŒ")
    catalog_ready = False

# ì‹¤ì œ ìœˆë„ìš° ìƒì„±
if combined_data is not None and catalog_ready:
    print("ìœˆë„ìš° ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ...")
    event_windows, event_labels, event_metadata = create_earthquake_windows(
        combined_data, catalog, sampling_rate, window_samples
    )
else:
    print("âš ï¸ ìœˆë„ìš° ìƒì„± ì¡°ê±´ ë¯¸ì¶©ì¡±")
    event_windows = np.array([])
    event_labels = []
    event_metadata = []

    print(f"\nâœ… ì´ë²¤íŠ¸ ê¸°ë°˜ ìœˆë„ìš° ìƒì„± ì™„ë£Œ:")
    print(f"  ğŸ“Š ì´ ìœˆë„ìš° ìˆ˜: {len(event_windows)}")
    if len(event_windows) > 0:
        print(f"  ğŸ“Š ìœˆë„ìš° shape: {event_windows[0].shape}")
        print(f"  ğŸ“Š ì „ì²´ shape: {event_windows.shape}")

# ============================================================================
# 4ë‹¨ê³„: ë°°ê²½ ë…¸ì´ì¦ˆ ìœˆë„ìš° ìƒì„± (ì§€ì§„ì´ ì—†ëŠ” êµ¬ê°„)
# ============================================================================
print(f"\nğŸŒŠ 4ë‹¨ê³„: ë°°ê²½ ë…¸ì´ì¦ˆ ìœˆë„ìš° ìƒì„±")

def create_background_windows(combined_data, window_samples, overlap_samples,
                             exclude_ranges=None):
    """ë°°ê²½ ë…¸ì´ì¦ˆ ìœˆë„ìš° ìƒì„± (ì§€ì§„ì´ ì—†ëŠ” êµ¬ê°„)"""

    background_windows = []
    background_metadata = []

    # ì „ì²´ ë°ì´í„°ì—ì„œ ì§€ì§„ êµ¬ê°„ì„ ì œì™¸í•œ ë¶€ë¶„ì—ì„œ ìœˆë„ìš° ìƒì„±
    total_samples = len(combined_data)

    # ì§€ì§„ êµ¬ê°„ ì œì™¸ (ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ì¤‘ì•™ 1/3 êµ¬ê°„ì„ ì§€ì§„ êµ¬ê°„ìœ¼ë¡œ ê°€ì •)
    exclude_start = total_samples // 3
    exclude_end = total_samples * 2 // 3

    print(f"ë°°ê²½ ë…¸ì´ì¦ˆ êµ¬ê°„:")
    print(f"  ğŸ”¸ êµ¬ê°„ 1: 0 ~ {exclude_start} samples")
    print(f"  ğŸ”¸ êµ¬ê°„ 2: {exclude_end} ~ {total_samples} samples")

    # ì²« ë²ˆì§¸ êµ¬ê°„ì—ì„œ ìœˆë„ìš° ìƒì„±
    for i in range(0, exclude_start - window_samples, overlap_samples):
        window = combined_data[i:i + window_samples]
        if len(window) == window_samples:
            background_windows.append(window)
            background_metadata.append({
                'window_start': i,
                'window_end': i + window_samples,
                'type': 'background_1'
            })

    # ë‘ ë²ˆì§¸ êµ¬ê°„ì—ì„œ ìœˆë„ìš° ìƒì„±
    for i in range(exclude_end, total_samples - window_samples, overlap_samples):
        window = combined_data[i:i + window_samples]
        if len(window) == window_samples:
            background_windows.append(window)
            background_metadata.append({
                'window_start': i,
                'window_end': i + window_samples,
                'type': 'background_2'
            })

    return np.array(background_windows), background_metadata

# ë°°ê²½ ë…¸ì´ì¦ˆ ìœˆë„ìš° ìƒì„±
if combined_data is not None:
    background_windows, background_metadata = create_background_windows(
        combined_data, window_samples, overlap_samples
    )

    print(f"âœ… ë°°ê²½ ë…¸ì´ì¦ˆ ìœˆë„ìš° ìƒì„± ì™„ë£Œ:")
    print(f"  ğŸ“Š ë°°ê²½ ìœˆë„ìš° ìˆ˜: {len(background_windows)}")
    if len(background_windows) > 0:
        print(f"  ğŸ“Š ìœˆë„ìš° shape: {background_windows[0].shape}")

# ============================================================================
# 5ë‹¨ê³„: ë…¸ì´ì¦ˆ ì œê±°ìš© ë°ì´í„° ìŒ ìƒì„±
# ============================================================================
print(f"\nğŸ¯ 5ë‹¨ê³„: ë…¸ì´ì¦ˆ ì œê±°ìš© ë°ì´í„° ìŒ ìƒì„±")

def add_realistic_noise(clean_windows, noise_level=0.1):
    """í˜„ì‹¤ì ì¸ ë…¸ì´ì¦ˆë¥¼ clean ì‹ í˜¸ì— ì¶”ê°€"""

    noisy_windows = []

    print(f"ë…¸ì´ì¦ˆ ì¶”ê°€ ì§„í–‰:")
    print(f"  ğŸ”Š ë…¸ì´ì¦ˆ ë ˆë²¨: {noise_level}")

    for i, clean_window in enumerate(clean_windows):
        # 1. ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ
        gaussian_noise = np.random.normal(0, noise_level * np.std(clean_window),
                                        clean_window.shape)

        # 2. ì „ë ¥ì„  ê°„ì„­ (60Hz)
        time_axis = np.arange(len(clean_window)) / sampling_rate
        power_line_noise = 0.05 * noise_level * np.sin(2 * np.pi * 60 * time_axis)

        # 3. ì €ì£¼íŒŒ ë“œë¦¬í”„íŠ¸
        drift_noise = 0.02 * noise_level * np.sin(2 * np.pi * 0.1 * time_axis)

        # ê° ì±„ë„ì— ë‹¤ë¥¸ ë…¸ì´ì¦ˆ íŠ¹ì„± ì ìš©
        noisy_window = clean_window.copy()
        for ch in range(clean_window.shape[1]):
            channel_noise = (gaussian_noise[:, ch] +
                           power_line_noise * (0.5 + 0.5 * ch) +  # ì±„ë„ë³„ ë‹¤ë¥¸ ê°•ë„
                           drift_noise[:, ch] if len(drift_noise.shape) > 1
                           else np.broadcast_to(drift_noise, (len(drift_noise),)))
            noisy_window[:, ch] += channel_noise

        noisy_windows.append(noisy_window)

        if (i + 1) % 10 == 0 or i == len(clean_windows) - 1:
            print(f"    ì§„í–‰ë¥ : {i+1}/{len(clean_windows)} ({(i+1)/len(clean_windows)*100:.1f}%)")

    return np.array(noisy_windows)

# Clean ë°ì´í„°ë¡œ ì´ë²¤íŠ¸ ìœˆë„ìš° ì‚¬ìš©
if 'event_windows' in locals() and len(event_windows) > 0:
    clean_data = event_windows
    noisy_data = add_realistic_noise(clean_data, noise_level=0.15)

    print(f"âœ… ë…¸ì´ì¦ˆ ì œê±°ìš© ë°ì´í„° ìŒ ìƒì„± ì™„ë£Œ:")
    print(f"  ğŸ“Š Clean ë°ì´í„°: {clean_data.shape}")
    print(f"  ğŸ“Š Noisy ë°ì´í„°: {noisy_data.shape}")

    # ë…¸ì´ì¦ˆ ì¶”ê°€ íš¨ê³¼ í™•ì¸
    print(f"  ğŸ“ˆ ë…¸ì´ì¦ˆ ì¶”ê°€ íš¨ê³¼:")
    print(f"    Clean í‘œì¤€í¸ì°¨: {np.std(clean_data):.4f}")
    print(f"    Noisy í‘œì¤€í¸ì°¨: {np.std(noisy_data):.4f}")
    print(f"    ë…¸ì´ì¦ˆ ë¹„ìœ¨: {(np.std(noisy_data) - np.std(clean_data))/np.std(clean_data)*100:.1f}%")

# ============================================================================
# 6ë‹¨ê³„: ìµœì¢… ì •ê·œí™” ë° ë°ì´í„°ì…‹ êµ¬ì„±
# ============================================================================
print(f"\nğŸ“ 6ë‹¨ê³„: ìµœì¢… ì •ê·œí™” ë° ë°ì´í„°ì…‹ êµ¬ì„±")

def normalize_data(data, method='z_score'):
    """ë°ì´í„° ì •ê·œí™”"""
    if method == 'z_score':
        mean = np.mean(data)
        std = np.std(data)
        normalized = (data - mean) / (std + 1e-8)  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
    elif method == 'min_max':
        min_val = np.min(data)
        max_val = np.max(data)
        normalized = (data - min_val) / (max_val - min_val + 1e-8)

    return normalized, {'mean': mean if method == 'z_score' else min_val,
                       'scale': std if method == 'z_score' else (max_val - min_val)}

# ë°ì´í„° ì •ê·œí™”
if 'noisy_data' in locals() and 'clean_data' in locals():
    print("ë°ì´í„° ì •ê·œí™” ì§„í–‰:")

    # ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•´ í†µê³„ ê³„ì‚°
    all_data = np.concatenate([noisy_data.flatten(), clean_data.flatten()])

    # Z-score ì •ê·œí™”
    normalized_noisy, norm_stats = normalize_data(noisy_data, method='z_score')
    normalized_clean, _ = normalize_data(clean_data, method='z_score')

    print(f"  âœ… Z-score ì •ê·œí™” ì™„ë£Œ")
    print(f"    í‰ê· : {norm_stats['mean']:.6f}")
    print(f"    í‘œì¤€í¸ì°¨: {norm_stats['scale']:.6f}")
    print(f"    ì •ê·œí™” í›„ ë²”ìœ„: {normalized_noisy.min():.3f} ~ {normalized_noisy.max():.3f}")

# ============================================================================
# 7ë‹¨ê³„: í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• 
# ============================================================================
print(f"\nğŸ“š 7ë‹¨ê³„: í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• ")

def split_dataset(X, y, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):
    """ë°ì´í„°ì…‹ì„ train/validation/testë¡œ ë¶„í• """

    total_samples = len(X)

    # ì¸ë±ìŠ¤ ì„ê¸°
    indices = np.random.permutation(total_samples)

    # ë¶„í•  ì§€ì  ê³„ì‚°
    train_end = int(total_samples * train_ratio)
    val_end = int(total_samples * (train_ratio + val_ratio))

    # ë¶„í• 
    train_idx = indices[:train_end]
    val_idx = indices[train_end:val_end]
    test_idx = indices[val_end:]

    return {
        'train': {'X': X[train_idx], 'y': y[train_idx], 'indices': train_idx},
        'val': {'X': X[val_idx], 'y': y[val_idx], 'indices': val_idx},
        'test': {'X': X[test_idx], 'y': y[test_idx], 'indices': test_idx}
    }

# ë°ì´í„° ë¶„í• 
if 'normalized_noisy' in locals() and 'normalized_clean' in locals():
    # ì‹œë“œ ì„¤ì •ìœ¼ë¡œ ì¬í˜„ ê°€ëŠ¥í•œ ë¶„í• 
    np.random.seed(42)

    dataset = split_dataset(normalized_noisy, normalized_clean,
                          train_ratio=0.7, val_ratio=0.2, test_ratio=0.1)

    print(f"âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
    for split_name, split_data in dataset.items():
        print(f"  ğŸ“Š {split_name.upper()}: {len(split_data['X'])}ê°œ ìƒ˜í”Œ")
        print(f"    X shape: {split_data['X'].shape}")
        print(f"    y shape: {split_data['y'].shape}")

# ============================================================================
# ìµœì¢… ê²°ê³¼ ìš”ì•½
# ============================================================================
print(f"\nğŸ‰ ë”¥ëŸ¬ë‹ìš© ì „ì²˜ë¦¬ ì™„ë£Œ!")
print("="*60)

if 'dataset' in locals():
    print(f"ğŸ“Š ìµœì¢… ë°ì´í„°ì…‹:")
    print(f"  ğŸ¯ ì‘ì—…: ì§€ì§„íŒŒ ë…¸ì´ì¦ˆ ì œê±° (Denoising)")
    print(f"  ğŸ“ ì…ë ¥ ì°¨ì›: {dataset['train']['X'].shape[1:]} (ì‹œê°„ x ì±„ë„)")
    print(f"  ğŸ“ˆ ë°ì´í„° ì •ê·œí™”: Z-score")
    print(f"  ğŸ”€ ë°ì´í„° ë¶„í• :")
    for split_name, split_data in dataset.items():
        ratio = len(split_data['X']) / (len(dataset['train']['X']) +
                                      len(dataset['val']['X']) +
                                      len(dataset['test']['X'])) * 100
        print(f"    - {split_name.upper()}: {len(split_data['X'])}ê°œ ({ratio:.1f}%)")

    print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„: ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì¤€ë¹„ ì™„ë£Œ!")
    print(f"  ğŸ’¡ ì¶”ì²œ ëª¨ë¸: U-Net, Autoencoder, or Transformer-based denoiser")
    print(f"  ğŸ“ ì‚¬ìš©ë²•:")
    print(f"    X_train = dataset['train']['X']")
    print(f"    y_train = dataset['train']['y']")
    print(f"    # ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
else:
    print(f"âŒ ì¼ë¶€ ë‹¨ê³„ ì‹¤íŒ¨ - ë””ë²„ê¹… í•„ìš”")

print(f"\nâœ… ë”¥ëŸ¬ë‹ìš© ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! ğŸŠ")



## 3. ê²°ê³¼ë°ì´í„° íŒŒì¼ì €ì¥

import pandas as pd
import numpy as np

print("ğŸ“‹ ë”¥ëŸ¬ë‹ ì „ì²˜ë¦¬ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥")
print("="*50)

def save_earthquake_data_to_csv(dataset):
    """ë”¥ëŸ¬ë‹ ì „ì²˜ë¦¬ëœ ì§€ì§„íŒŒ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥"""

    def reshape_and_save(data_X, data_y, filename_prefix):
        """3D ì§€ì§„íŒŒ ë°ì´í„°ë¥¼ 2D CSVë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥"""

        print(f"ğŸ’¾ {filename_prefix} ì„¸íŠ¸ ì €ì¥ ì¤‘...")

        n_samples, n_time, n_channels = data_X.shape
        print(f"  ğŸ“Š í˜•íƒœ: {n_samples}ê°œ ìƒ˜í”Œ Ã— {n_time}ì‹œì  Ã— {n_channels}ì±„ë„")

        # === X ë°ì´í„° (ë…¸ì´ì¦ˆ ìˆëŠ” ë°ì´í„°) ì €ì¥ ===
        # (1664, 800, 3) â†’ (1664, 2400) í˜•íƒœë¡œ ë³€í™˜
        X_reshaped = data_X.reshape(n_samples, -1)

        # ì»¬ëŸ¼ëª… ìƒì„±: t0_ch0, t0_ch1, t0_ch2, t1_ch0, t1_ch1, t1_ch2, ...
        X_columns = []
        for t in range(n_time):
            for ch in range(n_channels):
                channel_name = ['BHZ', 'BH1', 'BH2'][ch]  # ì‹¤ì œ ì±„ë„ëª… ì‚¬ìš©
                X_columns.append(f't{t}_{channel_name}')

        # DataFrame ìƒì„± ë° ì €ì¥
        X_df = pd.DataFrame(X_reshaped, columns=X_columns)
        X_df.to_csv(f'{filename_prefix}_X_noisy.csv', index=False)
        print(f"  âœ… {filename_prefix}_X_noisy.csv ì €ì¥ì™„ë£Œ ({X_df.shape})")

        # === y ë°ì´í„° (ê¹¨ë—í•œ ë°ì´í„°) ì €ì¥ ===
        y_reshaped = data_y.reshape(n_samples, -1)

        # ê°™ì€ ì»¬ëŸ¼ëª… ì‚¬ìš©
        y_columns = X_columns  # ë™ì¼í•œ êµ¬ì¡°

        y_df = pd.DataFrame(y_reshaped, columns=y_columns)
        y_df.to_csv(f'{filename_prefix}_y_clean.csv', index=False)
        print(f"  âœ… {filename_prefix}_y_clean.csv ì €ì¥ì™„ë£Œ ({y_df.shape})")

        return X_df.shape, y_df.shape

    # ê° ë°ì´í„°ì…‹ ì €ì¥
    total_saved = 0

    for split_name in ['train', 'val', 'test']:
        if split_name in dataset and len(dataset[split_name]['X']) > 0:
            X_shape, y_shape = reshape_and_save(
                dataset[split_name]['X'],
                dataset[split_name]['y'],
                split_name
            )
            total_saved += X_shape[0]
        else:
            print(f"âš ï¸ {split_name} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    return total_saved

# ë©”íƒ€ë°ì´í„° ì €ì¥
def save_metadata_csv(dataset):
    """ë°ì´í„°ì…‹ ë©”íƒ€ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥"""

    print(f"\nğŸ“Š ë©”íƒ€ë°ì´í„° ì €ì¥ ì¤‘...")

    # ê¸°ë³¸ ì •ë³´
    metadata = []

    for split_name in ['train', 'val', 'test']:
        if split_name in dataset:
            split_data = dataset[split_name]
            metadata.append({
                'split': split_name,
                'samples': len(split_data['X']),
                'time_steps': split_data['X'].shape[1] if len(split_data['X']) > 0 else 0,
                'channels': split_data['X'].shape[2] if len(split_data['X']) > 0 else 0,
                'total_features': split_data['X'].shape[1] * split_data['X'].shape[2] if len(split_data['X']) > 0 else 0
            })

    # ì „ì²´ ì •ë³´ ì¶”ê°€
    total_samples = sum(len(dataset[split]['X']) for split in ['train', 'val', 'test'] if split in dataset)

    metadata.append({
        'split': 'TOTAL',
        'samples': total_samples,
        'time_steps': 800,
        'channels': 3,
        'total_features': 2400
    })

    # ì„¤ì • ì •ë³´ ì¶”ê°€
    settings = pd.DataFrame([
        {'parameter': 'sampling_rate', 'value': '40 Hz'},
        {'parameter': 'window_duration', 'value': '20 seconds'},
        {'parameter': 'normalization', 'value': 'Z-score'},
        {'parameter': 'noise_level', 'value': '15%'},
        {'parameter': 'original_events', 'value': '1664'},
        {'parameter': 'channels', 'value': 'BHZ, BH1, BH2'},
        {'parameter': 'data_split', 'value': '70% train, 20% val, 10% test'}
    ])

    # ì €ì¥
    metadata_df = pd.DataFrame(metadata)
    metadata_df.to_csv('dataset_metadata.csv', index=False)
    settings.to_csv('dataset_settings.csv', index=False)

    print(f"  âœ… dataset_metadata.csv ì €ì¥ì™„ë£Œ")
    print(f"  âœ… dataset_settings.csv ì €ì¥ì™„ë£Œ")

    return metadata_df

# ì‹¤í–‰
if 'dataset' in locals():
    print(f"ğŸ¯ í˜„ì¬ ë°ì´í„°ì…‹ ìƒíƒœ:")
    for split_name in ['train', 'val', 'test']:
        if split_name in dataset:
            print(f"  ğŸ“š {split_name}: {len(dataset[split_name]['X'])}ê°œ ìƒ˜í”Œ")

    # CSV ì €ì¥ ì‹¤í–‰
    total_saved = save_earthquake_data_to_csv(dataset)
    metadata_df = save_metadata_csv(dataset)

    print(f"\nğŸ‰ CSV ì €ì¥ ì™„ë£Œ!")
    print(f"  ğŸ“Š ì´ ì €ì¥ëœ ìƒ˜í”Œ: {total_saved}ê°œ")
    print(f"  ğŸ“„ ìƒì„±ëœ íŒŒì¼ë“¤:")
    print(f"    - train_X_noisy.csv (ë…¸ì´ì¦ˆ ìˆëŠ” í›ˆë ¨ ë°ì´í„°)")
    print(f"    - train_y_clean.csv (ê¹¨ë—í•œ í›ˆë ¨ ë°ì´í„°)")
    print(f"    - val_X_noisy.csv (ë…¸ì´ì¦ˆ ìˆëŠ” ê²€ì¦ ë°ì´í„°)")
    print(f"    - val_y_clean.csv (ê¹¨ë—í•œ ê²€ì¦ ë°ì´í„°)")
    print(f"    - test_X_noisy.csv (ë…¸ì´ì¦ˆ ìˆëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°)")
    print(f"    - test_y_clean.csv (ê¹¨ë—í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°)")
    print(f"    - dataset_metadata.csv (ë°ì´í„°ì…‹ ì •ë³´)")
    print(f"    - dataset_settings.csv (ì„¤ì • ì •ë³´)")

    # íŒŒì¼ í¬ê¸° í™•ì¸
    import os
    print(f"\nğŸ“ íŒŒì¼ í¬ê¸°:")
    csv_files = [
        'train_X_noisy.csv', 'train_y_clean.csv',
        'val_X_noisy.csv', 'val_y_clean.csv',
        'test_X_noisy.csv', 'test_y_clean.csv'
    ]

    for file in csv_files:
        if os.path.exists(file):
            size_mb = os.path.getsize(file) / 1024 / 1024
            print(f"  ğŸ“„ {file}: {size_mb:.1f} MB")

    print(f"\nğŸ’¡ ì‚¬ìš©ë²•:")
    print(f"  # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°")
    print(f"  train_X = pd.read_csv('train_X_noisy.csv')")
    print(f"  train_y = pd.read_csv('train_y_clean.csv')")
    print(f"  # ë”¥ëŸ¬ë‹ í•™ìŠµì— ì‚¬ìš©!")

else:
    print("âŒ 'dataset' ë³€ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("ğŸ’¡ ë¨¼ì € ë”¥ëŸ¬ë‹ ì „ì²˜ë¦¬ë¥¼ ì™„ë£Œí•´ì£¼ì„¸ìš”.")



## ë”¥ëŸ¬ë‹ìš©ìœ¼ë¡œ ë³€í™˜
import pandas as pd
import numpy as np

print("ğŸš€ CSV â†’ ë”¥ëŸ¬ë‹ ë°ì´í„° ë³€í™˜ ì‹œì‘!")

def csv_to_deeplearning_ready(csv_prefix_list=['train', 'val', 'test']):
    """CSVì—ì„œ ë”¥ëŸ¬ë‹ ì¤€ë¹„ ì™„ë£Œ ë°ì´í„°ë¡œ í•œ ë²ˆì— ë³€í™˜"""

    dataset = {}

    for prefix in csv_prefix_list:
        print(f"ğŸ”„ {prefix} ë°ì´í„° ë¡œë”© ì¤‘...")

        try:
            # CSV íŒŒì¼ ì½ê¸°
            X_file = f'{prefix}_X_noisy.csv'
            y_file = f'{prefix}_y_clean.csv'

            X_df = pd.read_csv(X_file)
            y_df = pd.read_csv(y_file)

            # 3D ë³€í™˜: (ìƒ˜í”Œ, 2400) â†’ (ìƒ˜í”Œ, 800, 3)
            n_samples = len(X_df)
            X_3d = X_df.values.reshape(n_samples, 800, 3)
            y_3d = y_df.values.reshape(n_samples, 800, 3)

            # ë”•ì…”ë„ˆë¦¬ì— ì €ì¥
            dataset[prefix] = {
                'X': X_3d,  # ë…¸ì´ì¦ˆ ìˆëŠ” ë°ì´í„°
                'y': y_3d   # ê¹¨ë—í•œ ë°ì´í„°
            }

            print(f"  âœ… {prefix}: {X_3d.shape} â†’ {y_3d.shape}")

        except FileNotFoundError:
            print(f"  âŒ {prefix} íŒŒì¼ë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"  âŒ {prefix} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    return dataset

# ì‹¤í–‰!
dl_dataset = csv_to_deeplearning_ready()

# ê²°ê³¼ í™•ì¸
if dl_dataset:
    print(f"\nğŸ‰ ë³€í™˜ ì™„ë£Œ!")
    for split_name, data in dl_dataset.items():
        print(f"  ğŸ“Š {split_name}: {data['X'].shape} (ë…¸ì´ì¦ˆ) â†’ {data['y'].shape} (ê¹¨ë—í•¨)")

    print(f"\nğŸ’¡ ì‚¬ìš©ë²•:")
    print(f"  train_X = dl_dataset['train']['X']")
    print(f"  train_y = dl_dataset['train']['y']")
    print(f"  # ì´ì œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì— ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥! ğŸš€")
else:
    print("âŒ ë³€í™˜ ì‹¤íŒ¨")
